{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A02: Words, Transducers, Language Models, and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    By Besnik Balaj & Amanda Ly\n",
    "I plege my honor that I have abided by the Stevens Honor System \n",
    "                      -Amanda Ly & Besnik Balaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "1. Respond to J&M 2nd Exercises 3.6, 3.8, and 4.3. You are welcome to use existing n-gram programs. If you can find packages and procedures in software that apply you can use them but report on where you got them.\n",
    "2. From BKL for Python 3 (online) submit the results of the Your Turn practice on p 69 and 70 and Exercise 2.8.5, 2.8.14 (possibly used in disambiguation), and 2.8.27 (for only verbs) on pages 74-77. The concordance tool was first used on p4. You should become quite familiar with the use of WordNet and its companion SentiWordNet.\n",
    "3. For later work relevant to Chapters 10-13 on part-of-speech parsing find the longest sentence in one of the corpora: Moby Dick, Sense and Sensibility, Inaugural Address Corpus, The Wall Street Journal, and The Man Who Was Thursday, downloaded from instructions on page 2 and 3 of BKL. In effect find the longest string between consecutive periods. \n",
    "4. Search for perplexity measures (Section 3.7 in J&M 3rd) in Python and compare perplexity to lexical diversity, as specified on page 9 of BKL, on the corpus downloaded from the Python NLTK in the previous exercise. What does each measure? Is there a potential relationship and, if so, what is it? Is there a relationship of these notions to information content? This is not a pure programming exercise. \n",
    "5. You should interpret what each means in your own words. For perplexity calculation of 10-digit unigram numbers (check formulas so you agree with the calculations). See PerplexityUnigram10DigitNumber.xlsx.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. J&M 2nd Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.6\n",
    "Read Porter (1980) or see Martin Poerter's official homepage on the porter stemmer. \n",
    "Implement one of the steps of the Porter Stemmer as a transducer.\n",
    "\n",
    "    **See handwritten notes. for this case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.8\n",
    "Write a program that takes a word and, using an on-line dictionary, \n",
    "computes possible anagrams of the word, each of which is a legal word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def perms(elements):\n",
    "    if len(elements) <=1:\n",
    "        return elements\n",
    "    else:\n",
    "        tmp = []\n",
    "        for perm in all_perms(elements[1:]):\n",
    "            for i in range(len(elements)):\n",
    "                tmp.append(perm[:i] + elements[0:1] + perm[i:])\n",
    "        return tmp\n",
    "\n",
    "def wordGram(word):\n",
    "    legalWrds = []\n",
    "    #given a word make as many legal anagrams\n",
    "    possibleWords = perms(word)\n",
    "    for i in possibleWords:\n",
    "        if wordnet.synsets(i):\n",
    "            legalWrds.append(i)\n",
    "    print(legalWrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_perms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f498a49bbc49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordGram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hell'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-686db47e7289>\u001b[0m in \u001b[0;36mwordGram\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlegalWrds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#given a word make as many legal anagrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpossibleWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibleWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-686db47e7289>\u001b[0m in \u001b[0;36mperms\u001b[0;34m(elements)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mperm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_perms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_perms' is not defined"
     ]
    }
   ],
   "source": [
    "wordGram('Hell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Exercise 4.3\n",
    "Run your N-gram program on two different small corpora of your choice \n",
    "(you might use email text or newsgroups). Now compare the statistics of the two corpora. \n",
    "What are the differences in the most common unigrams between the two? \n",
    "How about interesting differences in bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This corpus is just a long text I am using from a email\n",
    "\n",
    "testEmail = \"Hi, Designers and product visionaries both ask the same question: How can I go from prototype to finished, polished product?We’d like to help if we can. Fluid UI has partnered with Developerfair.com to create an interactive tool which will guide you through some basic questions to help you get a sense for your project. Fill out the super quick questionnaire and we promise to follow up with a personalised reply within 24-72 hours with a list of tips for your project.We\\'ll also suggesting some possible next steps, including the kind of resources you might need and how much it is likely to cost.\"\n",
    "\n",
    "testEmail2 = \"GraphConnect 2020 is just around the corner (April 20-22 in New York City), and we’re calling on all graphistas to submit their presentation ideas to share with attendees. The Neo4j community has connected so many people who use graphs in incredible ways. Now it’s your turn to share your experiences with others. We look forward to reading your proposals! Thanks,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtained from StackOverflow here\n",
    "#https://stackoverflow.com/questions/13423919/computing-n-grams-using-python\n",
    "def ngrams2(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = {}\n",
    "    for i in range(len(input)-n+1):\n",
    "        g = ' '.join(input[i:i+n])\n",
    "        output.setdefault(g,0)\n",
    "        output[g] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams2(str(text1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams2(testEmail,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams2(testEmail2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB = str(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "ngrams2(MB,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams2(str(gutenberg.sents('shakespeare-macbeth.txt')),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = ngrams(testEmail.split(), 2)\n",
    "unigrams = ngrams(testEmail.split(), 1)\n",
    "for grams in bigrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(testEmail2.split(), 2)\n",
    "unigrams = ngrams(testEmail2.split(), 1)\n",
    "for grams in bigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Discussion:\n",
    "Depending on the corpus being used can change both the unigram and bigram amount. Of course with regards to the unigrams\n",
    "we can see the use of common words such as the/to/a/of/etc. But its with some other words we can see differences. Neo4j is not\n",
    "Fluid UI. This is where we can see some specific choices leading to guess what corpus came from. With regards to bigrams\n",
    "A bigram can help express some matter of the field of the topic for both. Both try to attract the user to their own\n",
    "thing but in a different way. The words found in the text2 are more leanning to bring someone somewhere while text1 was\n",
    "to send a mission statement. With a larger corpus, we can see the numbers behind some cases but acquiring those have proven difficult.\n",
    "\n",
    "UPDATE:\n",
    "I did a quick run on Moby Dick as seen above and noticed that some wordplay on the bigrams are\n",
    "time dependent or social dependent. What I mean is that Shakespeare follow up words\n",
    "are going to be different from Melville's due to the time period since some words existed or\n",
    "meant different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Your Turn (page 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down all the senses of the word *dish* that you can\n",
    "think of. Now, explore this word with the help of WordNet, using the\n",
    "same operations shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('dish.n.05').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('dish.n.05').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('dish.n.05').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('dish.n.05').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemma('dish.n.05.dish_aerial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemma('dish.n.05.dish_aerial').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemma('dish.n.05.dish_aerial').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('dish'):\n",
    "    print (synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.lemmas('dish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Your Turn (page 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out NLTK’s convenient graphical WordNet browser:\n",
    "nltk.app.wordnet(). Explore the WordNet hierarchy by following the\n",
    "hypernym and hyponym links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('dark.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade = wn.synset('dark.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_shade = shade.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_shade[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what happened here??!?!??!!\n",
    "sorted([lemma.name for synset in the types_of_shade for lemma in synset.lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = shade.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Exercise 2.8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Exercise 2.8.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **supergloss(s)** that takes a synset **s** as its argument and returns a string consisting of the concatenation of the definition of **s**, and the definitions of all the hypernyms and hyponyms of **s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Exercise 2.8.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polysemy of a word is the number of senses it has. Using WordNet, we can\n",
    "determine that the noun *dog* has seven senses with **len(wn.synsets('dog', 'n'))**.\n",
    "Compute the average polysemy of nouns, verbs, adjectives, and adverbs according\n",
    "to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the Longest String in Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "260819\n",
      "to\n",
      "10059\n",
      "['Though', 'in', 'many', 'natural', 'objects', ',', 'whiteness', 'refiningly', 'enhances', 'beauty', ',', 'as', 'if', 'imparting', 'some', 'special', 'virtue', 'of', 'its', 'own', ',', 'as', 'in', 'marbles', ',', 'japonicas', ',', 'and', 'pearls', ';', 'and', 'though', 'various', 'nations', 'have', 'in', 'some', 'way', 'recognised', 'a', 'certain', 'royal', 'preeminence', 'in', 'this', 'hue', ';', 'even', 'the', 'barbaric', ',', 'grand', 'old', 'kings', 'of', 'Pegu', 'placing', 'the', 'title', '\"', 'Lord', 'of', 'the', 'White', 'Elephants', '\"', 'above', 'all', 'their', 'other', 'magniloquent', 'ascriptions', 'of', 'dominion', ';', 'and', 'the', 'modern', 'kings', 'of', 'Siam', 'unfurling', 'the', 'same', 'snow', '-', 'white', 'quadruped', 'in', 'the', 'royal', 'standard', ';', 'and', 'the', 'Hanoverian', 'flag', 'bearing', 'the', 'one', 'figure', 'of', 'a', 'snow', '-', 'white', 'charger', ';', 'and', 'the', 'great', 'Austrian', 'Empire', ',', 'Caesarian', ',', 'heir', 'to', 'overlording', 'Rome', ',', 'having', 'for', 'the', 'imperial', 'colour', 'the', 'same', 'imperial', 'hue', ';', 'and', 'though', 'this', 'pre', '-', 'eminence', 'in', 'it', 'applies', 'to', 'the', 'human', 'race', 'itself', ',', 'giving', 'the', 'white', 'man', 'ideal', 'mastership', 'over', 'every', 'dusky', 'tribe', ';', 'and', 'though', ',', 'besides', ',', 'all', 'this', ',', 'whiteness', 'has', 'been', 'even', 'made', 'significant', 'of', 'gladness', ',', 'for', 'among', 'the', 'Romans', 'a', 'white', 'stone', 'marked', 'a', 'joyful', 'day', ';', 'and', 'though', 'in', 'other', 'mortal', 'sympathies', 'and', 'symbolizings', ',', 'this', 'same', 'hue', 'is', 'made', 'the', 'emblem', 'of', 'many', 'touching', ',', 'noble', 'things', '--', 'the', 'innocence', 'of', 'brides', ',', 'the', 'benignity', 'of', 'age', ';', 'though', 'among', 'the', 'Red', 'Men', 'of', 'America', 'the', 'giving', 'of', 'the', 'white', 'belt', 'of', 'wampum', 'was', 'the', 'deepest', 'pledge', 'of', 'honour', ';', 'though', 'in', 'many', 'climes', ',', 'whiteness', 'typifies', 'the', 'majesty', 'of', 'Justice', 'in', 'the', 'ermine', 'of', 'the', 'Judge', ',', 'and', 'contributes', 'to', 'the', 'daily', 'state', 'of', 'kings', 'and', 'queens', 'drawn', 'by', 'milk', '-', 'white', 'steeds', ';', 'though', 'even', 'in', 'the', 'higher', 'mysteries', 'of', 'the', 'most', 'august', 'religions', 'it', 'has', 'been', 'made', 'the', 'symbol', 'of', 'the', 'divine', 'spotlessness', 'and', 'power', ';', 'by', 'the', 'Persian', 'fire', 'worshippers', ',', 'the', 'white', 'forked', 'flame', 'being', 'held', 'the', 'holiest', 'on', 'the', 'altar', ';', 'and', 'in', 'the', 'Greek', 'mythologies', ',', 'Great', 'Jove', 'himself', 'being', 'made', 'incarnate', 'in', 'a', 'snow', '-', 'white', 'bull', ';', 'and', 'though', 'to', 'the', 'noble', 'Iroquois', ',', 'the', 'midwinter', 'sacrifice', 'of', 'the', 'sacred', 'White', 'Dog', 'was', 'by', 'far', 'the', 'holiest', 'festival', 'of', 'their', 'theology', ',', 'that', 'spotless', ',', 'faithful', 'creature', 'being', 'held', 'the', 'purest', 'envoy', 'they', 'could', 'send', 'to', 'the', 'Great', 'Spirit', 'with', 'the', 'annual', 'tidings', 'of', 'their', 'own', 'fidelity', ';', 'and', 'though', 'directly', 'from', 'the', 'Latin', 'word', 'for', 'white', ',', 'all', 'Christian', 'priests', 'derive', 'the', 'name', 'of', 'one', 'part', 'of', 'their', 'sacred', 'vesture', ',', 'the', 'alb', 'or', 'tunic', ',', 'worn', 'beneath', 'the', 'cassock', ';', 'and', 'though', 'among', 'the', 'holy', 'pomps', 'of', 'the', 'Romish', 'faith', ',', 'white', 'is', 'specially', 'employed', 'in', 'the', 'celebration', 'of', 'the', 'Passion', 'of', 'our', 'Lord', ';', 'though', 'in', 'the', 'Vision', 'of', 'St', '.', 'John', ',', 'white', 'robes', 'are', 'given', 'to', 'the', 'redeemed', ',', 'and', 'the', 'four', '-', 'and', '-', 'twenty', 'elders', 'stand', 'clothed', 'in', 'white', 'before', 'the', 'great', '-', 'white', 'throne', ',', 'and', 'the', 'Holy', 'One', 'that', 'sitteth', 'there', 'white', 'like', 'wool', ';', 'yet', 'for', 'all', 'these', 'accumulated', 'associations', ',', 'with', 'whatever', 'is', 'sweet', ',', 'and', 'honourable', ',', 'and', 'sublime', ',', 'there', 'yet', 'lurks', 'an', 'elusive', 'something', 'in', 'the', 'innermost', 'idea', 'of', 'this', 'hue', ',', 'which', 'strikes', 'more', 'of', 'panic', 'to', 'the', 'soul', 'than', 'that', 'redness', 'which', 'affrights', 'in', 'blood', '.']\n",
      "542\n",
      "Though in many natural objects , whiteness refiningly enhances beauty , as if imparting some special virtue of its own , as in marbles , japonicas , and pearls ; and though various nations have in some way recognised a certain royal preeminence in this hue ; even the barbaric , grand old kings of Pegu placing the title \" Lord of the White Elephants \" above all their other magniloquent ascriptions of dominion ; and the modern kings of Siam unfurling the same snow - white quadruped in the royal standard ; and the Hanoverian flag bearing the one figure of a snow - white charger ; and the great Austrian Empire , Caesarian , heir to overlording Rome , having for the imperial colour the same imperial hue ; and though this pre - eminence in it applies to the human race itself , giving the white man ideal mastership over every dusky tribe ; and though , besides , all this , whiteness has been even made significant of gladness , for among the Romans a white stone marked a joyful day ; and though in other mortal sympathies and symbolizings , this same hue is made the emblem of many touching , noble things -- the innocence of brides , the benignity of age ; though among the Red Men of America the giving of the white belt of wampum was the deepest pledge of honour ; though in many climes , whiteness typifies the majesty of Justice in the ermine of the Judge , and contributes to the daily state of kings and queens drawn by milk - white steeds ; though even in the higher mysteries of the most august religions it has been made the symbol of the divine spotlessness and power ; by the Persian fire worshippers , the white forked flame being held the holiest on the altar ; and in the Greek mythologies , Great Jove himself being made incarnate in a snow - white bull ; and though to the noble Iroquois , the midwinter sacrifice of the sacred White Dog was by far the holiest festival of their theology , that spotless , faithful creature being held the purest envoy they could send to the Great Spirit with the annual tidings of their own fidelity ; and though directly from the Latin word for white , all Christian priests derive the name of one part of their sacred vesture , the alb or tunic , worn beneath the cassock ; and though among the holy pomps of the Romish faith , white is specially employed in the celebration of the Passion of our Lord ; though in the Vision of St . John , white robes are given to the redeemed , and the four - and - twenty elders stand clothed in white before the great - white throne , and the Holy One that sitteth there white like wool ; yet for all these accumulated associations , with whatever is sweet , and honourable , and sublime , there yet lurks an elusive something in the innermost idea of this hue , which strikes more of panic to the soul than that redness which affrights in blood .\n"
     ]
    }
   ],
   "source": [
    "#CITE: https://www.nltk.org/book/ch02.html\n",
    "MB = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "print(type(MB))\n",
    "print(len(MB))\n",
    "print(MB[101])\n",
    "#for i in range(len(MB)):\n",
    "    #if (MB[i] == '.'):\n",
    "        #print('RAWR')\n",
    "#Can use bottom for easy way of finding sentences\n",
    "#macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "#So this creates an array of arrays, where the inner array contains the words present in the sentence\n",
    "\n",
    "Moby_Dick_Sent = gutenberg.sents('melville-moby_dick.txt')\n",
    "print(len(Moby_Dick_Sent))\n",
    "longest = 0\n",
    "for i in Moby_Dick_Sent:\n",
    "    if (len(i) >= longest):\n",
    "        longest = len(i)\n",
    "        LongSent = i;\n",
    "\n",
    "print(LongSent)\n",
    "print(len(LongSent))\n",
    "ans = ' '.join(LongSent)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Search Perplixity measures in Python and compare it to Lexical Diversity. What do each measure , are they related somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.502044830977896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The lexical Diversity \n",
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))\n",
    "lexical_diversity(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/33266956/nltk-package-to-estimate-the-unigram-perplexity\n",
    "\n",
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "MB = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "tokens = nltk.word_tokenize(str(MB))\n",
    "\n",
    "def unigram(tokens):\n",
    "    model = collections.defaultdict(lambda: 0.1)\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model[f] = 1\n",
    "            continue\n",
    "    N = float(sum(model.values()))\n",
    "    for word in model:\n",
    "        model[word] = model[word]/N\n",
    "    return model\n",
    "#here you construct the unigram language model \n",
    "\n",
    "\n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99999999999997\n"
     ]
    }
   ],
   "source": [
    "model = unigram(tokens)\n",
    "print (perplexity(str(MB), model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

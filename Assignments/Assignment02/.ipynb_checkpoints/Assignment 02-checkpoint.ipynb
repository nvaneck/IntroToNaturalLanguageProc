{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A02: Words, Transducers, Language Models, and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    By Besnik Balaj & Amanda Ly\n",
    "I plege my honor that I have abided by the Stevens Honor System \n",
    "                      -Amanda Ly & Besnik Balaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "1. Respond to J&M 2nd Exercises 3.6, 3.8, and 4.3. You are welcome to use existing n-gram programs. If you can find packages and procedures in software that apply you can use them but report on where you got them.\n",
    "2. From BKL for Python 3 (online) submit the results of the Your Turn practice on p 69 and 70 and Exercise 2.8.5, 2.8.14 (possibly used in disambiguation), and 2.8.27 (for only verbs) on pages 74-77. The concordance tool was first used on p4. You should become quite familiar with the use of WordNet and its companion SentiWordNet.\n",
    "3. For later work relevant to Chapters 10-13 on part-of-speech parsing find the longest sentence in one of the corpora: Moby Dick, Sense and Sensibility, Inaugural Address Corpus, The Wall Street Journal, and The Man Who Was Thursday, downloaded from instructions on page 2 and 3 of BKL. In effect find the longest string between consecutive periods. \n",
    "4. Search for perplexity measures (Section 3.7 in J&M 3rd) in Python and compare perplexity to lexical diversity, as specified on page 9 of BKL, on the corpus downloaded from the Python NLTK in the previous exercise. What does each measure? Is there a potential relationship and, if so, what is it? Is there a relationship of these notions to information content? This is not a pure programming exercise. \n",
    "5. You should interpret what each means in your own words. For perplexity calculation of 10-digit unigram numbers (check formulas so you agree with the calculations). See PerplexityUnigram10DigitNumber.xlsx.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exercise 3.6\n",
    "Read Porter (1980) or see Martin Poerter's official homepage on the porter stemmer. \n",
    "Implement one of the steps of the Porter Stemmer as a transducer.\n",
    "'''\n",
    "#See handwritten notes. for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exercise 3.8\n",
    "Write a program that takes a word and, using an on-line dictionary, \n",
    "computes possible anagrams of the word, each of which is a legal word.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def perms(elements):\n",
    "    if len(elements) <=1:\n",
    "        return elements\n",
    "    else:\n",
    "        tmp = []\n",
    "        for perm in all_perms(elements[1:]):\n",
    "            for i in range(len(elements)):\n",
    "                tmp.append(perm[:i] + elements[0:1] + perm[i:])\n",
    "        return tmp\n",
    "\n",
    "def wordGram(word):\n",
    "    legalWrds = []\n",
    "    #given a word make as many legal anagrams\n",
    "    possibleWords = perms(word)\n",
    "    for i in possibleWords:\n",
    "        if wordnet.synsets(i):\n",
    "            legalWrds.append(i)\n",
    "    print(legalWrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'Hell']\n"
     ]
    }
   ],
   "source": [
    "wordGram('Hell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExercise 4.3\\nRun your N-gram program on two different small corpora of your choice \\n(you might use email text or newsgroups). Now compare the statistics of the two corpora. \\nWhat are the differences in the most common unigrams between the two? \\nHow about interesting differences in bigrams?\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Exercise 4.3\n",
    "Run your N-gram program on two different small corpora of your choice \n",
    "(you might use email text or newsgroups). Now compare the statistics of the two corpora. \n",
    "What are the differences in the most common unigrams between the two? \n",
    "How about interesting differences in bigrams?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This corpus is just a long text I am using from a email\n",
    "\n",
    "testEmail = \"Hi, Designers and product visionaries both ask the same question: How can I go from prototype to finished, polished product?We’d like to help if we can. Fluid UI has partnered with Developerfair.com to create an interactive tool which will guide you through some basic questions to help you get a sense for your project. Fill out the super quick questionnaire and we promise to follow up with a personalised reply within 24-72 hours with a list of tips for your project.We\\'ll also suggesting some possible next steps, including the kind of resources you might need and how much it is likely to cost.\"\n",
    "\n",
    "testEmail2 = \"GraphConnect 2020 is just around the corner (April 20-22 in New York City), and we’re calling on all graphistas to submit their presentation ideas to share with attendees. The Neo4j community has connected so many people who use graphs in incredible ways. Now it’s your turn to share your experiences with others. We look forward to reading your proposals! Thanks,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtained from StackOverflow here\n",
    "#https://stackoverflow.com/questions/13423919/computing-n-grams-using-python\n",
    "def ngrams2(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = {}\n",
    "    for i in range(len(input)-n+1):\n",
    "        g = ' '.join(input[i:i+n])\n",
    "        output.setdefault(g,0)\n",
    "        output[g] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<Text: Moby': 1,\n",
       " 'Moby Dick': 1,\n",
       " 'Dick by': 1,\n",
       " 'by Herman': 1,\n",
       " 'Herman Melville': 1,\n",
       " 'Melville 1851>': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2(str(text1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hi, Designers': 1,\n",
       " 'Designers and': 1,\n",
       " 'and product': 1,\n",
       " 'product visionaries': 1,\n",
       " 'visionaries both': 1,\n",
       " 'both ask': 1,\n",
       " 'ask the': 1,\n",
       " 'the same': 1,\n",
       " 'same question:': 1,\n",
       " 'question: How': 1,\n",
       " 'How can': 1,\n",
       " 'can I': 1,\n",
       " 'I go': 1,\n",
       " 'go from': 1,\n",
       " 'from prototype': 1,\n",
       " 'prototype to': 1,\n",
       " 'to finished,': 1,\n",
       " 'finished, polished': 1,\n",
       " 'polished product?We’d': 1,\n",
       " 'product?We’d like': 1,\n",
       " 'like to': 1,\n",
       " 'to help': 2,\n",
       " 'help if': 1,\n",
       " 'if we': 1,\n",
       " 'we can.': 1,\n",
       " 'can. Fluid': 1,\n",
       " 'Fluid UI': 1,\n",
       " 'UI has': 1,\n",
       " 'has partnered': 1,\n",
       " 'partnered with': 1,\n",
       " 'with Developerfair.com': 1,\n",
       " 'Developerfair.com to': 1,\n",
       " 'to create': 1,\n",
       " 'create an': 1,\n",
       " 'an interactive': 1,\n",
       " 'interactive tool': 1,\n",
       " 'tool which': 1,\n",
       " 'which will': 1,\n",
       " 'will guide': 1,\n",
       " 'guide you': 1,\n",
       " 'you through': 1,\n",
       " 'through some': 1,\n",
       " 'some basic': 1,\n",
       " 'basic questions': 1,\n",
       " 'questions to': 1,\n",
       " 'help you': 1,\n",
       " 'you get': 1,\n",
       " 'get a': 1,\n",
       " 'a sense': 1,\n",
       " 'sense for': 1,\n",
       " 'for your': 2,\n",
       " 'your project.': 1,\n",
       " 'project. Fill': 1,\n",
       " 'Fill out': 1,\n",
       " 'out the': 1,\n",
       " 'the super': 1,\n",
       " 'super quick': 1,\n",
       " 'quick questionnaire': 1,\n",
       " 'questionnaire and': 1,\n",
       " 'and we': 1,\n",
       " 'we promise': 1,\n",
       " 'promise to': 1,\n",
       " 'to follow': 1,\n",
       " 'follow up': 1,\n",
       " 'up with': 1,\n",
       " 'with a': 2,\n",
       " 'a personalised': 1,\n",
       " 'personalised reply': 1,\n",
       " 'reply within': 1,\n",
       " 'within 24-72': 1,\n",
       " '24-72 hours': 1,\n",
       " 'hours with': 1,\n",
       " 'a list': 1,\n",
       " 'list of': 1,\n",
       " 'of tips': 1,\n",
       " 'tips for': 1,\n",
       " \"your project.We'll\": 1,\n",
       " \"project.We'll also\": 1,\n",
       " 'also suggesting': 1,\n",
       " 'suggesting some': 1,\n",
       " 'some possible': 1,\n",
       " 'possible next': 1,\n",
       " 'next steps,': 1,\n",
       " 'steps, including': 1,\n",
       " 'including the': 1,\n",
       " 'the kind': 1,\n",
       " 'kind of': 1,\n",
       " 'of resources': 1,\n",
       " 'resources you': 1,\n",
       " 'you might': 1,\n",
       " 'might need': 1,\n",
       " 'need and': 1,\n",
       " 'and how': 1,\n",
       " 'how much': 1,\n",
       " 'much it': 1,\n",
       " 'it is': 1,\n",
       " 'is likely': 1,\n",
       " 'likely to': 1,\n",
       " 'to cost.': 1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2(testEmail,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GraphConnect 2020': 1,\n",
       " '2020 is': 1,\n",
       " 'is just': 1,\n",
       " 'just around': 1,\n",
       " 'around the': 1,\n",
       " 'the corner': 1,\n",
       " 'corner (April': 1,\n",
       " '(April 20-22': 1,\n",
       " '20-22 in': 1,\n",
       " 'in New': 1,\n",
       " 'New York': 1,\n",
       " 'York City),': 1,\n",
       " 'City), and': 1,\n",
       " 'and we’re': 1,\n",
       " 'we’re calling': 1,\n",
       " 'calling on': 1,\n",
       " 'on all': 1,\n",
       " 'all graphistas': 1,\n",
       " 'graphistas to': 1,\n",
       " 'to submit': 1,\n",
       " 'submit their': 1,\n",
       " 'their presentation': 1,\n",
       " 'presentation ideas': 1,\n",
       " 'ideas to': 1,\n",
       " 'to share': 2,\n",
       " 'share with': 1,\n",
       " 'with attendees.': 1,\n",
       " 'attendees. The': 1,\n",
       " 'The Neo4j': 1,\n",
       " 'Neo4j community': 1,\n",
       " 'community has': 1,\n",
       " 'has connected': 1,\n",
       " 'connected so': 1,\n",
       " 'so many': 1,\n",
       " 'many people': 1,\n",
       " 'people who': 1,\n",
       " 'who use': 1,\n",
       " 'use graphs': 1,\n",
       " 'graphs in': 1,\n",
       " 'in incredible': 1,\n",
       " 'incredible ways.': 1,\n",
       " 'ways. Now': 1,\n",
       " 'Now it’s': 1,\n",
       " 'it’s your': 1,\n",
       " 'your turn': 1,\n",
       " 'turn to': 1,\n",
       " 'share your': 1,\n",
       " 'your experiences': 1,\n",
       " 'experiences with': 1,\n",
       " 'with others.': 1,\n",
       " 'others. We': 1,\n",
       " 'We look': 1,\n",
       " 'look forward': 1,\n",
       " 'forward to': 1,\n",
       " 'to reading': 1,\n",
       " 'reading your': 1,\n",
       " 'your proposals!': 1,\n",
       " 'proposals! Thanks,': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2(testEmail2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams2(str(gutenberg.sents('shakespeare-macbeth.txt')),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi,', 'Designers')\n",
      "('Designers', 'and')\n",
      "('and', 'product')\n",
      "('product', 'visionaries')\n",
      "('visionaries', 'both')\n",
      "('both', 'ask')\n",
      "('ask', 'the')\n",
      "('the', 'same')\n",
      "('same', 'question:')\n",
      "('question:', 'How')\n",
      "('How', 'can')\n",
      "('can', 'I')\n",
      "('I', 'go')\n",
      "('go', 'from')\n",
      "('from', 'prototype')\n",
      "('prototype', 'to')\n",
      "('to', 'finished,')\n",
      "('finished,', 'polished')\n",
      "('polished', 'product?We’d')\n",
      "('product?We’d', 'like')\n",
      "('like', 'to')\n",
      "('to', 'help')\n",
      "('help', 'if')\n",
      "('if', 'we')\n",
      "('we', 'can.')\n",
      "('can.', 'Fluid')\n",
      "('Fluid', 'UI')\n",
      "('UI', 'has')\n",
      "('has', 'partnered')\n",
      "('partnered', 'with')\n",
      "('with', 'Developerfair.com')\n",
      "('Developerfair.com', 'to')\n",
      "('to', 'create')\n",
      "('create', 'an')\n",
      "('an', 'interactive')\n",
      "('interactive', 'tool')\n",
      "('tool', 'which')\n",
      "('which', 'will')\n",
      "('will', 'guide')\n",
      "('guide', 'you')\n",
      "('you', 'through')\n",
      "('through', 'some')\n",
      "('some', 'basic')\n",
      "('basic', 'questions')\n",
      "('questions', 'to')\n",
      "('to', 'help')\n",
      "('help', 'you')\n",
      "('you', 'get')\n",
      "('get', 'a')\n",
      "('a', 'sense')\n",
      "('sense', 'for')\n",
      "('for', 'your')\n",
      "('your', 'project.')\n",
      "('project.', 'Fill')\n",
      "('Fill', 'out')\n",
      "('out', 'the')\n",
      "('the', 'super')\n",
      "('super', 'quick')\n",
      "('quick', 'questionnaire')\n",
      "('questionnaire', 'and')\n",
      "('and', 'we')\n",
      "('we', 'promise')\n",
      "('promise', 'to')\n",
      "('to', 'follow')\n",
      "('follow', 'up')\n",
      "('up', 'with')\n",
      "('with', 'a')\n",
      "('a', 'personalised')\n",
      "('personalised', 'reply')\n",
      "('reply', 'within')\n",
      "('within', '24-72')\n",
      "('24-72', 'hours')\n",
      "('hours', 'with')\n",
      "('with', 'a')\n",
      "('a', 'list')\n",
      "('list', 'of')\n",
      "('of', 'tips')\n",
      "('tips', 'for')\n",
      "('for', 'your')\n",
      "('your', \"project.We'll\")\n",
      "(\"project.We'll\", 'also')\n",
      "('also', 'suggesting')\n",
      "('suggesting', 'some')\n",
      "('some', 'possible')\n",
      "('possible', 'next')\n",
      "('next', 'steps,')\n",
      "('steps,', 'including')\n",
      "('including', 'the')\n",
      "('the', 'kind')\n",
      "('kind', 'of')\n",
      "('of', 'resources')\n",
      "('resources', 'you')\n",
      "('you', 'might')\n",
      "('might', 'need')\n",
      "('need', 'and')\n",
      "('and', 'how')\n",
      "('how', 'much')\n",
      "('much', 'it')\n",
      "('it', 'is')\n",
      "('is', 'likely')\n",
      "('likely', 'to')\n",
      "('to', 'cost.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = ngrams(testEmail.split(), 2)\n",
    "unigrams = ngrams(testEmail.split(), 1)\n",
    "for grams in bigrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi,',)\n",
      "('Designers',)\n",
      "('and',)\n",
      "('product',)\n",
      "('visionaries',)\n",
      "('both',)\n",
      "('ask',)\n",
      "('the',)\n",
      "('same',)\n",
      "('question:',)\n",
      "('How',)\n",
      "('can',)\n",
      "('I',)\n",
      "('go',)\n",
      "('from',)\n",
      "('prototype',)\n",
      "('to',)\n",
      "('finished,',)\n",
      "('polished',)\n",
      "('product?We’d',)\n",
      "('like',)\n",
      "('to',)\n",
      "('help',)\n",
      "('if',)\n",
      "('we',)\n",
      "('can.',)\n",
      "('Fluid',)\n",
      "('UI',)\n",
      "('has',)\n",
      "('partnered',)\n",
      "('with',)\n",
      "('Developerfair.com',)\n",
      "('to',)\n",
      "('create',)\n",
      "('an',)\n",
      "('interactive',)\n",
      "('tool',)\n",
      "('which',)\n",
      "('will',)\n",
      "('guide',)\n",
      "('you',)\n",
      "('through',)\n",
      "('some',)\n",
      "('basic',)\n",
      "('questions',)\n",
      "('to',)\n",
      "('help',)\n",
      "('you',)\n",
      "('get',)\n",
      "('a',)\n",
      "('sense',)\n",
      "('for',)\n",
      "('your',)\n",
      "('project.',)\n",
      "('Fill',)\n",
      "('out',)\n",
      "('the',)\n",
      "('super',)\n",
      "('quick',)\n",
      "('questionnaire',)\n",
      "('and',)\n",
      "('we',)\n",
      "('promise',)\n",
      "('to',)\n",
      "('follow',)\n",
      "('up',)\n",
      "('with',)\n",
      "('a',)\n",
      "('personalised',)\n",
      "('reply',)\n",
      "('within',)\n",
      "('24-72',)\n",
      "('hours',)\n",
      "('with',)\n",
      "('a',)\n",
      "('list',)\n",
      "('of',)\n",
      "('tips',)\n",
      "('for',)\n",
      "('your',)\n",
      "(\"project.We'll\",)\n",
      "('also',)\n",
      "('suggesting',)\n",
      "('some',)\n",
      "('possible',)\n",
      "('next',)\n",
      "('steps,',)\n",
      "('including',)\n",
      "('the',)\n",
      "('kind',)\n",
      "('of',)\n",
      "('resources',)\n",
      "('you',)\n",
      "('might',)\n",
      "('need',)\n",
      "('and',)\n",
      "('how',)\n",
      "('much',)\n",
      "('it',)\n",
      "('is',)\n",
      "('likely',)\n",
      "('to',)\n",
      "('cost.',)\n"
     ]
    }
   ],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GraphConnect', '2020')\n",
      "('2020', 'is')\n",
      "('is', 'just')\n",
      "('just', 'around')\n",
      "('around', 'the')\n",
      "('the', 'corner')\n",
      "('corner', '(April')\n",
      "('(April', '20-22')\n",
      "('20-22', 'in')\n",
      "('in', 'New')\n",
      "('New', 'York')\n",
      "('York', 'City),')\n",
      "('City),', 'and')\n",
      "('and', 'we’re')\n",
      "('we’re', 'calling')\n",
      "('calling', 'on')\n",
      "('on', 'all')\n",
      "('all', 'graphistas')\n",
      "('graphistas', 'to')\n",
      "('to', 'submit')\n",
      "('submit', 'their')\n",
      "('their', 'presentation')\n",
      "('presentation', 'ideas')\n",
      "('ideas', 'to')\n",
      "('to', 'share')\n",
      "('share', 'with')\n",
      "('with', 'attendees.')\n",
      "('attendees.', 'The')\n",
      "('The', 'Neo4j')\n",
      "('Neo4j', 'community')\n",
      "('community', 'has')\n",
      "('has', 'connected')\n",
      "('connected', 'so')\n",
      "('so', 'many')\n",
      "('many', 'people')\n",
      "('people', 'who')\n",
      "('who', 'use')\n",
      "('use', 'graphs')\n",
      "('graphs', 'in')\n",
      "('in', 'incredible')\n",
      "('incredible', 'ways.')\n",
      "('ways.', 'Now')\n",
      "('Now', 'it’s')\n",
      "('it’s', 'your')\n",
      "('your', 'turn')\n",
      "('turn', 'to')\n",
      "('to', 'share')\n",
      "('share', 'your')\n",
      "('your', 'experiences')\n",
      "('experiences', 'with')\n",
      "('with', 'others.')\n",
      "('others.', 'We')\n",
      "('We', 'look')\n",
      "('look', 'forward')\n",
      "('forward', 'to')\n",
      "('to', 'reading')\n",
      "('reading', 'your')\n",
      "('your', 'proposals!')\n",
      "('proposals!', 'Thanks,')\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(testEmail2.split(), 2)\n",
    "unigrams = ngrams(testEmail2.split(), 1)\n",
    "for grams in bigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GraphConnect',)\n",
      "('2020',)\n",
      "('is',)\n",
      "('just',)\n",
      "('around',)\n",
      "('the',)\n",
      "('corner',)\n",
      "('(April',)\n",
      "('20-22',)\n",
      "('in',)\n",
      "('New',)\n",
      "('York',)\n",
      "('City),',)\n",
      "('and',)\n",
      "('we’re',)\n",
      "('calling',)\n",
      "('on',)\n",
      "('all',)\n",
      "('graphistas',)\n",
      "('to',)\n",
      "('submit',)\n",
      "('their',)\n",
      "('presentation',)\n",
      "('ideas',)\n",
      "('to',)\n",
      "('share',)\n",
      "('with',)\n",
      "('attendees.',)\n",
      "('The',)\n",
      "('Neo4j',)\n",
      "('community',)\n",
      "('has',)\n",
      "('connected',)\n",
      "('so',)\n",
      "('many',)\n",
      "('people',)\n",
      "('who',)\n",
      "('use',)\n",
      "('graphs',)\n",
      "('in',)\n",
      "('incredible',)\n",
      "('ways.',)\n",
      "('Now',)\n",
      "('it’s',)\n",
      "('your',)\n",
      "('turn',)\n",
      "('to',)\n",
      "('share',)\n",
      "('your',)\n",
      "('experiences',)\n",
      "('with',)\n",
      "('others.',)\n",
      "('We',)\n",
      "('look',)\n",
      "('forward',)\n",
      "('to',)\n",
      "('reading',)\n",
      "('your',)\n",
      "('proposals!',)\n",
      "('Thanks,',)\n"
     ]
    }
   ],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Discussion:\n",
    "Depending on the corpus being used can change \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Your Turn (page 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down all the senses of the word *dish* that you can\n",
    "think of. Now, explore this word with the help of WordNet, using the\n",
    "same operations shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dish', 'dish_aerial', 'dish_antenna', 'saucer']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dish.n.05').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dish.n.05').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dish.n.05').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.05.dish'),\n",
       " Lemma('dish.n.05.dish_aerial'),\n",
       " Lemma('dish.n.05.dish_antenna'),\n",
       " Lemma('dish.n.05.saucer')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dish.n.05').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('dish.n.05.dish_aerial')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('dish.n.05.dish_aerial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dish.n.05')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('dish.n.05.dish_aerial').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish_aerial'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('dish.n.05.dish_aerial').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish']\n",
      "['dish']\n",
      "['dish', 'dishful']\n",
      "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
      "['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
      "['cup_of_tea', 'bag', 'dish']\n",
      "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
      "['dish']\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dish'):\n",
    "    print (synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.01.dish'),\n",
       " Lemma('dish.n.02.dish'),\n",
       " Lemma('dish.n.03.dish'),\n",
       " Lemma('smasher.n.02.dish'),\n",
       " Lemma('dish.n.05.dish'),\n",
       " Lemma('cup_of_tea.n.01.dish'),\n",
       " Lemma('serve.v.06.dish'),\n",
       " Lemma('dish.v.02.dish')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('dish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Your Turn (page 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out NLTK’s convenient graphical WordNet browser:\n",
    "nltk.app.wordnet(). Explore the WordNet hierarchy by following the\n",
    "hypernym and hyponym links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.app.wordnet_app.app()>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.app.wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dark.n.01'),\n",
       " Synset('iniquity.n.01'),\n",
       " Synset('darkness.n.02'),\n",
       " Synset('night.n.01'),\n",
       " Synset('dark.n.05'),\n",
       " Synset('dark.a.01'),\n",
       " Synset('dark.a.02'),\n",
       " Synset('dark.s.03'),\n",
       " Synset('black.s.05'),\n",
       " Synset('dark.s.05'),\n",
       " Synset('dark.s.06'),\n",
       " Synset('benighted.s.02'),\n",
       " Synset('dark.s.08'),\n",
       " Synset('blue.s.08'),\n",
       " Synset('colored.s.02'),\n",
       " Synset('dark.s.11')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark', 'darkness']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dark.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "shade = wn.synset('dark.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_shade = shade.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('total_darkness.n.01')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_of_shade[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-160-05f848b79ad4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-160-05f848b79ad4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    sorted([lemma.name for synset in the types_of_shade for lemma in synset.lemmas])\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## what happened here??!?!??!!\n",
    "sorted([lemma.name for synset in the types_of_shade for lemma in synset.lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('illumination.n.02')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shade.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = shade.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'abstraction.n.06',\n",
       " 'attribute.n.02',\n",
       " 'state.n.02',\n",
       " 'illumination.n.02',\n",
       " 'dark.n.01']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shade.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Exercise 2.8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Exercise 2.8.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **supergloss(s)** that takes a synset **s** as its argument and returns a string consisting of the concatenation of the definition of **s**, and the definitions of all the hypernyms and hyponyms of **s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Exercise 2.8.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polysemy of a word is the number of senses it has. Using WordNet, we can\n",
    "determine that the noun *dog* has seven senses with **len(wn.synsets('dog', 'n'))**.\n",
    "Compute the average polysemy of nouns, verbs, adjectives, and adverbs according\n",
    "to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A03: Detecting Spelling Errors, Minimum Edit Distance, Human Morphology and Sentiment Analysis with Naïve Bayesian Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    By Besnik Balaj & Amanda Ly\n",
    "I plege my honor that I have abided by the Stevens Honor System \n",
    "                      -Amanda Ly & Besnik Balaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables: \n",
    "\n",
    "1. Respond to J&M 2nd Exercises 3.10 and 3.11.\n",
    "\n",
    "2. Find Python packages that apply Bayesian logic to classification and apply one to sentiment data. Such data can be downloaded from Amazon which collects reviews from customers, and possibly specialized rating sites for electronics, entertainment, restaurants, and other products and services. A sentiment data set using Amazon data is available with papers that report on research using it from here (Links to an external site.).\n",
    "\n",
    "3. Briefly report on SentiWordNet (Links to an external site.) and how it can be used with Python and WordNet to classify a corpus of reviews (tweets are not easy to get right since they do not use “natural” language). Do not get bogged down preparing the reviews for analysis. It is acceptable to find a research paper (Links to an external site.) or blog that reports on such use.\n",
    "\n",
    "4. Respond to the Bayesian part of BKL Exercise 6.3. Entire exercise can be a project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to Look:\n",
    "- J&M 2nd Edition on Chapter 3 \n",
    "- J&M 3rd Edition on Chapter 4\n",
    "- A&S Chapter 2 \n",
    "- BKL Section 6.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. J&M 2nd Edition Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.10 :\n",
    "Computing Minimum Edit Distance by hand, find out whether drive is closer to brief or to divers, and what the edit distance is. You may use any version of distance you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations:\n",
    "\n",
    "    D[i,j] = MIN { D[i−1,j] +1 \n",
    "                    D[i,j−1] +1\n",
    "                    D[i−1,j−1] + {2; if source[i] /= target[j]\n",
    "                                  0; if source[i] = target[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, I am using insertion cost 1, deletion cost 1, and subsitution cost 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Edit Distance Grid: drive vs divers\n",
    " \n",
    "          d  r  i  v  e\n",
    "       0  1  2  3  4  5\n",
    "    d  1  0  1  2  3  4  \n",
    "    i  2  1  2  1  2  3      \n",
    "    v  3  2  3  2  1  2    \n",
    "    e  4  3  4  3  2  1    \n",
    "    r  5  4  3  4  3  2\n",
    "    s  6  5  4  5  4  3\n",
    " \n",
    " The minimum edit distance is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Distance Grid: drive vs divers\n",
    " \n",
    "          d  r  i  v  e\n",
    "       0  1  2  3  4  5\n",
    "    b  1  2  3  4  5  6      \n",
    "    r  2  3  2  3  4  5      \n",
    "    i  3  4  3  2  3  4      \n",
    "    e  4  5  4  3  4  3     \n",
    "    f  5  6  5  4  5  4\n",
    "The minimum edit distance is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore divers is closer to drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.11:\n",
    "Now implement a minimum edit distance algorithim and use your hand computed results to check your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From DRIVE to BRIEF we achieve an Edit Distance of 3\n",
      "From DRIVE to DIVERS we achieve an Edit Distance of 3\n",
      "So here we see an issue of assuming Replacement to be the cost of just 1 instead of 2 (where it is a combination of Delete and insert). We see DRIVE is possibly similar to both equally but that can't be the case.So let us see what happens from our handwork above. When DRIVE goes to BRIEF we are Deleting D and Inserting B followed by a Deletion of V and Insertion of F.From DRIVE to DIVERS we have a Deleting of R, an Inserting of R and an Inserting of S. Thus 1 less step and specifcally a less perplex step of an immediate removal and insert. Therefore the perplexity would need to be taken account.\n"
     ]
    }
   ],
   "source": [
    "#https://www.geeksforgeeks.org/edit-distance-dp-5/ <--Where I got code but wish to update something\n",
    "def editDistance(str1, str2, m , n): \n",
    "    if m==0: \n",
    "         return n \n",
    "    if n==0: \n",
    "        return m \n",
    "    if str1[m-1]==str2[n-1]: \n",
    "        return editDistance(str1,str2,m-1,n-1) \n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert \n",
    "                   editDistance(str1, str2, m-1, n),    # Remove \n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace \n",
    "                   ) \n",
    "\n",
    "str1 = \"DRIVE\"\n",
    "str2 = \"BRIEF\"\n",
    "str3 = \"DIVERS\"\n",
    "DtoB = editDistance(str1, str2, len(str1), len(str2))\n",
    "DtoD = editDistance(str1, str3, len(str1), len(str3))\n",
    "print(f\"From DRIVE to BRIEF we achieve an Edit Distance of {DtoB}\")\n",
    "print(f\"From DRIVE to DIVERS we achieve an Edit Distance of {DtoD}\")\n",
    "print(\"So here we see an issue of assuming Replacement to be the cost of just 1 instead of 2 (where it is a combination of Delete and insert). We see DRIVE is possibly similar to both equally but that can't be the case.\"\n",
    "      \"So let us see what happens from our handwork above. When DRIVE goes to BRIEF we are Deleting D and Inserting B followed by a Deletion of V and Insertion of F.\"\n",
    "      \"From DRIVE to DIVERS we have a Deleting of R, an Inserting of R and an Inserting of S. Thus 1 less step and specifcally a less perplex step of an immediate removal and insert. Therefore the perplexity would need to be taken account.\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From DRIVE to BRIEF is 3.0\n",
      "From DRIVE to DIVERS is 3.0\n"
     ]
    }
   ],
   "source": [
    "#https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "#Another implementation here\n",
    "import numpy as np\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "print(f\"From DRIVE to BRIEF is {levenshtein(str1,str2)}\")\n",
    "print(f\"From DRIVE to DIVERS is {levenshtein(str1,str3)}\")\n",
    "print(\"This is just another case of cost being set to 1, an incorrect thought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From DRIVE to BRIEF is 4\n",
      "From DRIVE to DIVERS is 3\n",
      "And here we can see that when we assume cost to potentially be 2 for subsition or 0, we get a clearer and correct re-sult\n"
     ]
    }
   ],
   "source": [
    "def LD(s, t):\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    if s[-1] == t[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 2\n",
    "       \n",
    "    res = min([LD(s[:-1], t)+1,\n",
    "               LD(s, t[:-1])+1, \n",
    "               LD(s[:-1], t[:-1]) + cost])\n",
    "    return res\n",
    "print(f\"From DRIVE to BRIEF is {LD(str1,str2)}\")\n",
    "print(f\"From DRIVE to DIVERS is {LD(str1,str3)}\")\n",
    "print(\"And here we can see that when we assume cost to potentially be 2 for subsition or 0, we get a clearer and correct re-sult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian Logic Package and Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Python packages that apply Bayesian logic to classification and apply one to sentiment data. Such data can be downloaded from Amazon which collects reviews from customers, and possibly specialized rating sites for electronics, entertainment, restaurants, and other products and services. A sentiment data set using Amazon data is available with papers that report on research using it from here (Links to an external site.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find package and import/get it here\n",
    "#Describe what it do ya kno and test it \n",
    "#link for tests: https://www.cs.jhu.edu/~mdredze/datasets/sentiment/ ITS TARS! YAYYYYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pymc3 as pm #This crap wasn't workin even after some installations\n",
    "##Alright im like 94% sure a naive Bayes exist within NLTK or Scikit etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "     contains(atrocious) = True              neg : pos    =     11.0 : 1.0\n",
      "       contains(martian) = True              neg : pos    =      7.7 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      7.0 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      7.0 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#This is an example of Bayesian Logic in Movie reviews\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)#randomize the doc to make sure model has chance to test and choose different ones\n",
    "#print(\"Documents\")\n",
    "#print(documents[0])\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "#print(all_words)\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100] #split up data, should be random but another test split function exists \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=oXZThwEF4r0 for a new \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviewsInText/Training.txt', sep='\\t', names=['liked','txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liked</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   liked                                                txt\n",
       "0      1            The Da Vinci Code book is just awesome.\n",
       "1      1  this was the first clive cussler i've ever rea...\n",
       "2      1                   i liked the Da Vinci Code a lot.\n",
       "3      1                   i liked the Da Vinci Code a lot.\n",
       "4      1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TD(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6918,)\n",
      "(6918, 2011)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979292333245913"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 1)\n",
      "('enjoyed', 1)\n",
      "('the', 1)\n",
      "('parts', 1)\n",
      "('regarding', 1)\n",
      "('the', 1)\n",
      "('S.S.', 1)\n",
      "('Valiant,', 1)\n",
      "('but', 1)\n",
      "('the', 1)\n",
      "('later', 0)\n",
      "('part', 1)\n",
      "('of', 1)\n",
      "('the', 1)\n",
      "('book', 1)\n",
      "('simply', 1)\n",
      "(\"didn't\", 1)\n",
      "('work', 0)\n",
      "('for', 1)\n",
      "('me.', 1)\n",
      "('Except', 0)\n",
      "('for', 1)\n",
      "('Picard,', 1)\n",
      "('who', 1)\n",
      "('we', 1)\n",
      "('of', 1)\n",
      "('course', 1)\n",
      "('know', 1)\n",
      "('quite', 1)\n",
      "('well', 1)\n",
      "('from', 1)\n",
      "('the', 1)\n",
      "('series,', 1)\n",
      "('the', 1)\n",
      "('other', 1)\n",
      "('characters', 0)\n",
      "('were', 1)\n",
      "('flat', 1)\n",
      "('and', 1)\n",
      "('uninteresting.', 1)\n",
      "('We', 1)\n",
      "('never', 1)\n",
      "('find', 1)\n",
      "('out', 1)\n",
      "('what', 1)\n",
      "('is', 1)\n",
      "('motivating', 1)\n",
      "('the', 1)\n",
      "('alien', 1)\n",
      "('attackers', 1)\n",
      "('and', 1)\n",
      "('they', 1)\n",
      "('are', 1)\n",
      "('the', 1)\n",
      "('cardboard', 1)\n",
      "('cutout', 1)\n",
      "('enemy-of-the-week,', 1)\n",
      "('boring.', 0)\n",
      "('The', 1)\n",
      "('whole', 0)\n",
      "('“adversarial”', 1)\n",
      "('situation', 1)\n",
      "('with', 1)\n",
      "('the', 1)\n",
      "('1st', 1)\n",
      "('Officer', 1)\n",
      "('also', 1)\n",
      "('just', 1)\n",
      "('struck', 1)\n",
      "('me', 1)\n",
      "('as', 1)\n",
      "('odd', 1)\n",
      "('and', 1)\n",
      "('wrong,', 1)\n",
      "('and', 1)\n",
      "('was', 1)\n",
      "('just', 1)\n",
      "('not', 1)\n",
      "('developed', 1)\n",
      "('correctly', 1)\n",
      "('or', 1)\n",
      "('fleshed', 1)\n",
      "('out', 1)\n",
      "('beyond', 1)\n",
      "('being', 1)\n",
      "('another', 1)\n",
      "('cardboard', 1)\n",
      "('enemy', 1)\n",
      "('for', 1)\n",
      "('Picard', 1)\n",
      "The amount of positive words were 84 while the negative words are 6 and thus we can conclude This review can be overall seen as positive\n"
     ]
    }
   ],
   "source": [
    "#Alright so let us pass through some testy sentences from the processed sentiment files\n",
    "#Specifically:\n",
    "with open(\"reviewsInText/review1.rtf\",\"r\") as f:\n",
    "    context = f.read()\n",
    "    holder = context.split(\" \")\n",
    "holder.remove('Helvetica;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;}\\n{\\\\*\\\\expandedcolortbl;;}\\n\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind0\\n\\\\pard\\\\tx720\\\\tx1440\\\\tx2160\\\\tx2880\\\\tx3600\\\\tx4320\\\\tx5040\\\\tx5760\\\\tx6480\\\\tx7200\\\\tx7920\\\\tx8640\\\\pardirnatural\\\\partightenfactor0\\n\\n\\\\f0\\\\fs24')\n",
    "holder.remove('{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1671\\\\cocoasubrtf600\\n{\\\\fonttbl\\\\f0\\\\fswiss\\\\fcharset0')\n",
    "holder.remove('\\\\cf0')\n",
    "#print(holder)\n",
    "holder = \"I enjoyed the parts regarding the S.S. Valiant, but the later part of the book simply didn't work for me. Except for Picard, who we of course know quite well from the series, the other characters were flat and uninteresting. We never find out what is motivating the alien attackers and they are the cardboard cutout enemy-of-the-week, boring. The whole “adversarial” situation with the 1st Officer also just struck me as odd and wrong, and was just not developed correctly or fleshed out beyond being another cardboard enemy for Picard\"\n",
    "holder2 = holder.split(\" \")\n",
    "#print(holder2)\n",
    "tester = vectorizer.transform(holder2)\n",
    "#print (clf.predict(tester))\n",
    "answers = clf.predict(tester)\n",
    "answers_hold = []\n",
    "pcnt = 0;\n",
    "ncnt = 0;\n",
    "for i in range(len(holder2)):\n",
    "    if i == 90:\n",
    "        break\n",
    "    answers_hold.append((holder2[i],answers[i]))\n",
    "    if (answers[i] == 1):\n",
    "        pcnt += 1\n",
    "    else:\n",
    "        ncnt += 1\n",
    "\n",
    "for i in answers_hold:\n",
    "    print(i)\n",
    "#NOTE: Objectivity is not assumed here, easier to assume that as positivity since we all proud and optimisitic\n",
    "Conclusion = 'This review can be overall seen as positive'\n",
    "Conclusion2 = 'This review can be overall seen as negative'\n",
    "Conclusion3 = 'It is too close to tell '\n",
    "if pcnt > ncnt:\n",
    "    true = Conclusion\n",
    "elif ncnt > pcnt:\n",
    "    true = Conclusion2\n",
    "else:\n",
    "    true = Conclusion3\n",
    "print(f\"The amount of positive words were {pcnt} while the negative words are {ncnt} and thus we can conclude {true}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#Will need to run some more tests on this, since top sentence felt more negative but maybe not understanding concept\n",
    "test2 = np.array(['The movie was a dumb movie'])\n",
    "test3 = np.array(['While the movie was dumb, it did have some neat action scenes'])\n",
    "tester = vectorizer.transform(test2)\n",
    "answers = clf.predict(tester)\n",
    "print(answers)\n",
    "tester = vectorizer.transform(test3)\n",
    "answers = clf.predict(tester)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Report on SentiWordNet and its use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly report on SentiWordNet (Links to an external site.) and how it can be used with Python and WordNet to classify a corpus of reviews (tweets are not easy to get right since they do not use “natural” language). Do not get bogged down preparing the reviews for analysis. It is acceptable to find a research paper (Links to an external site.) or blog that reports on such use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUT DISCUSSION HERE\n",
    "#LINK HERE: https://www.researchgate.net/publication/267249616_Reviews_Classification_Using_SentiWordNet_Lexicon RESEARCH!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CITES: http://www.nltk.org/howto/sentiwordnet.html <br>\n",
    "CITES: https://nlpforhackers.io/sentiment-analysis-intro/ <br>\n",
    "CITES: https://www.geeksforgeeks.org/nlp-synsets-for-a-word-in-wordnet/ <br> \n",
    "\n",
    "__Below is a small breakdown of how SentiWordNet works as Senti SynSets__ <br>\n",
    "swn.senti_synset('breakdown.n.03')<br>\n",
    "SentiSynSets Usage:<br>\n",
    "breakdown = swn.senti_synset('breakdown.n.03')<br>\n",
    "<breakdown.n.03: PosScore=0.0 NegScore=0.25><br>\n",
    "breakdown.pos_score(): 0.0<br>\n",
    "breakdown.neg_score(): 0.25<br>\n",
    "breakdown.obj_score(): 0.75<br>\n",
    "\n",
    "__Below is a small breakdown on how SentiWordNet works for Lookup__\n",
    "list(swn.senti_synsets('slow')) # doctest: +NORMALIZE_WHITESPACE <br>\n",
    "[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'),<br>\n",
    "SentiSynset('slow.v.03'), SentiSynset('slow.a.01'),<br>\n",
    "SentiSynset('slow.a.02'), SentiSynset('slow.a.04'),<br>\n",
    "SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]<br>\n",
    "happy = swn.senti_synsets('happy', 'a') <br>\n",
    "\n",
    "\n",
    "__Discussion on SentiWordNet and its importance on Corpus Reviews__\n",
    "SentiWordNet is useful in classifying a corpus of reviews due to its connection to WordNet already and the ability to be used in Python (a langauge with already strong bonds in machine learning). Firstly is to understand how Synset was previously used in current work of WordNet. Synset instances are the groupings of synonymous words that express the same concept but not enitrely meaning. So if a company wishes to review how everyone is feeling about a product (and make the machine do it instead) they need to understand the sentiment behind the word. This is where SentiWordNet becomes useful. It'll take the 3 instances:\n",
    "- breakdown' = word you need scores for.\n",
    "- 'n' = part of speech\n",
    "- '03' = Usage (01 for most common usage and a higher number would indicate lesser common usages)\n",
    "\n",
    "And yield scores to 3 categories. These categories are Positivie, Negative, Objective. The reasoning here is that one should be able to run through a corpus and per word (or most recurring word) gain an overall sense on if the reviews were postive/negative or neutral to a degree. So its cool for this to be done but now the reason becomes why? Well one can: \n",
    "- Use the reviews to understand how a product is doing in the current market and peoples reactions\n",
    "- automate customer support / ai to understand if a person is feeling fine/bad/neutral with current service and help\n",
    "- See overall reviews for the product and use it to better understand complicated corpuses that utilize \"unique\" languages \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bayesian part of BKL Exercise 6.3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Senseval 2 Corpus contains data intended to train word-sense disambigua-tion classifiers. It contains data for four words: hard, interest, line, and serve. Choose one of these four words, and load the corresponding data:  \n",
    "\n",
    "- from nltk.corpus import senseval\n",
    "- instances = senseval.instances('hard.pos')\n",
    "- size = int(len(instances) * 0.1)\n",
    "- train_set, test_set = instances[size:], instances[:size]\n",
    "\n",
    "Using this dataset, build a classifier that predicts the correct sense tag for a given instance. See the corpus HOWTO at http://www.nltk.org/howto for informationon using the instance objects returned by the Senseval 2 Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

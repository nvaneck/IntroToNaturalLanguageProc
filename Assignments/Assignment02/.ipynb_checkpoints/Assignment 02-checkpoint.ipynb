{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A02: Words, Transducers, Language Models, and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    By Besnik Balaj & Amanda Ly\n",
    "I plege my honor that I have abided by the Stevens Honor System \n",
    "                      -Amanda Ly & Besnik Balaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "1. Respond to J&M 2nd Exercises 3.6, 3.8, and 4.3. You are welcome to use existing n-gram programs. If you can find packages and procedures in software that apply you can use them but report on where you got them.\n",
    "2. From BKL for Python 3 (online) submit the results of the Your Turn practice on p 69 and 70 and Exercise 2.8.5, 2.8.14 (possibly used in disambiguation), and 2.8.27 (for only verbs) on pages 74-77. The concordance tool was first used on p4. You should become quite familiar with the use of WordNet and its companion SentiWordNet.\n",
    "3. For later work relevant to Chapters 10-13 on part-of-speech parsing find the longest sentence in one of the corpora: Moby Dick, Sense and Sensibility, Inaugural Address Corpus, The Wall Street Journal, and The Man Who Was Thursday, downloaded from instructions on page 2 and 3 of BKL. In effect find the longest string between consecutive periods. \n",
    "4. Search for perplexity measures (Section 3.7 in J&M 3rd) in Python and compare perplexity to lexical diversity, as specified on page 9 of BKL, on the corpus downloaded from the Python NLTK in the previous exercise. What does each measure? Is there a potential relationship and, if so, what is it? Is there a relationship of these notions to information content? This is not a pure programming exercise. You should interpret what each means in your own words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. J&M 2nd Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.6\n",
    "Read Porter (1980) or see Martin Poerter's official homepage on the porter stemmer. \n",
    "Implement one of the steps of the Porter Stemmer as a transducer.\n",
    "\n",
    "    **See handwritten notes. for this case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.8\n",
    "Write a program that takes a word and, using an on-line dictionary, \n",
    "computes possible anagrams of the word, each of which is a legal word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def perms(elements):\n",
    "    if len(elements) <=1:\n",
    "        return elements\n",
    "    else:\n",
    "        tmp = []\n",
    "        for perm in perms(elements[1:]):\n",
    "            for i in range(len(elements)):\n",
    "                tmp.append(perm[:i] + elements[0:1] + perm[i:])\n",
    "        return tmp\n",
    "\n",
    "def wordGram(word):\n",
    "    legalWrds = []\n",
    "    #given a word make as many legal anagrams\n",
    "    possibleWords = perms(word)\n",
    "    for i in possibleWords:\n",
    "        if wordnet.synsets(i):\n",
    "            legalWrds.append(i)\n",
    "    print(legalWrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'Hell']\n"
     ]
    }
   ],
   "source": [
    "wordGram('Hell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Exercise 4.3\n",
    "Run your N-gram program on two different small corpora of your choice \n",
    "(you might use email text or newsgroups). Now compare the statistics of the two corpora. \n",
    "What are the differences in the most common unigrams between the two? \n",
    "How about interesting differences in bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This corpus is just a long text I am using from a email\n",
    "\n",
    "testEmail = \"Hi, Designers and product visionaries both ask the same question: How can I go from prototype to finished, polished product?We’d like to help if we can. Fluid UI has partnered with Developerfair.com to create an interactive tool which will guide you through some basic questions to help you get a sense for your project. Fill out the super quick questionnaire and we promise to follow up with a personalised reply within 24-72 hours with a list of tips for your project.We\\'ll also suggesting some possible next steps, including the kind of resources you might need and how much it is likely to cost.\"\n",
    "\n",
    "testEmail2 = \"GraphConnect 2020 is just around the corner (April 20-22 in New York City), and we’re calling on all graphistas to submit their presentation ideas to share with attendees. The Neo4j community has connected so many people who use graphs in incredible ways. Now it’s your turn to share your experiences with others. We look forward to reading your proposals! Thanks,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtained from StackOverflow here\n",
    "#https://stackoverflow.com/questions/13423919/computing-n-grams-using-python\n",
    "def ngrams2(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = {}\n",
    "    for i in range(len(input)-n+1):\n",
    "        g = ' '.join(input[i:i+n])\n",
    "        output.setdefault(g,0)\n",
    "        output[g] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams2(str(text1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams2(testEmail,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams2(testEmail2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB = str(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "ngrams2(MB,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams2(str(gutenberg.sents('shakespeare-macbeth.txt')),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = ngrams(testEmail.split(), 2)\n",
    "unigrams = ngrams(testEmail.split(), 1)\n",
    "for grams in bigrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(testEmail2.split(), 2)\n",
    "unigrams = ngrams(testEmail2.split(), 1)\n",
    "for grams in bigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Discussion:\n",
    "Depending on the corpus being used can change both the unigram and bigram amount. Of course with regards to the unigrams\n",
    "we can see the use of common words such as the/to/a/of/etc. But its with some other words we can see differences. Neo4j is not\n",
    "Fluid UI. This is where we can see some specific choices leading to guess what corpus came from. With regards to bigrams\n",
    "A bigram can help express some matter of the field of the topic for both. Both try to attract the user to their own\n",
    "thing but in a different way. The words found in the text2 are more leanning to bring someone somewhere while text1 was\n",
    "to send a mission statement. With a larger corpus, we can see the numbers behind some cases but acquiring those have proven difficult.\n",
    "\n",
    "UPDATE:\n",
    "I did a quick run on Moby Dick as seen above and noticed that some wordplay on the bigrams are\n",
    "time dependent or social dependent. What I mean is that Shakespeare follow up words\n",
    "are going to be different from Melville's due to the time period since some words existed or\n",
    "meant different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Your Turn (page 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down all the senses of the word *dish* that you can\n",
    "think of. Now, explore this word with the help of WordNet, using the\n",
    "same operations shown earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Definitions: \n",
    "   1. dish(v) - the act of putting food onto a plate\n",
    "   2. dish(n) - a container to hold food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma_names:  ['dish']\n",
      "Definition:  a particular item of prepared food\n",
      "Examples:  ['she prepared a special dish for dinner']\n"
     ]
    }
   ],
   "source": [
    "#change the part in the parenthesis to the diff Synset options\n",
    "lemName = wn.synset('dish.n.02').lemma_names()\n",
    "defs = wn.synset('dish.n.02').definition()\n",
    "examples = wn.synset('dish.n.02').examples()\n",
    "print(\"Lemma_names: \", lemName)\n",
    "print(\"Definition: \", defs)\n",
    "print(\"Examples: \", examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.05.dish'),\n",
       " Lemma('dish.n.05.dish_aerial'),\n",
       " Lemma('dish.n.05.dish_antenna'),\n",
       " Lemma('dish.n.05.saucer')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the part in the parethesis to different Synset options \n",
    "wn.synset('dish.n.05').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('dish.n.05.dish_aerial')\n",
      "Synset('dish.n.05')\n",
      "dish_aerial\n"
     ]
    }
   ],
   "source": [
    "#Change the part in the parethesis to different Lemma options \n",
    "Lemma = wn.lemma('dish.n.05.dish_aerial')\n",
    "lemSyn = wn.lemma('dish.n.05.dish_aerial').synset()\n",
    "lemName = wn.lemma('dish.n.05.dish_aerial').name()\n",
    "print(Lemma)\n",
    "print(lemSyn)\n",
    "print(lemName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish']\n",
      "['dish']\n",
      "['dish', 'dishful']\n",
      "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
      "['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
      "['cup_of_tea', 'bag', 'dish']\n",
      "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
      "['dish']\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dish'):\n",
    "    print (synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.01.dish'),\n",
       " Lemma('dish.n.02.dish'),\n",
       " Lemma('dish.n.03.dish'),\n",
       " Lemma('smasher.n.02.dish'),\n",
       " Lemma('dish.n.05.dish'),\n",
       " Lemma('cup_of_tea.n.01.dish'),\n",
       " Lemma('serve.v.06.dish'),\n",
       " Lemma('dish.v.02.dish')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('dish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet Definitions: \n",
    "    1. S(n): dish-a piece of dishware normally used as a container for holding or serving food\n",
    "        e.g. we gave them a set of dishes for a wedding present    \n",
    "    2. S(n): dish-a particular item of prepared food\n",
    "        e.g. she prepared a special dish for dinner\n",
    "    3. S(n): dish/dishful-the quantity that a dish will hold\n",
    "        e.g. they served me a dish of rice\n",
    "    4. S(n): smasher/stunner/knockout/beauty/ravisher/sweetheart/peach/lulu/looker/mantrap/dish-a very attractive or seductive looking woman\n",
    "    5. S(n): dish/dish_aerial/dish_antenna/saucer-directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
    "    6. S(n): cup_of_tea/bag/dish-an activity that you like or at which you are superior\n",
    "        e.g. chemistry is not my cup of tea; his bag now is learning to play golf; marriage was scarcely his dish\n",
    "    7. S(v): serve/serve_up/dish_out/dish_up/dish-provide (usually but not necessarily food)\n",
    "        e.g. We serve meals for the homeless; She dished out the soup at 8 P.M.; The entertainers served up a lively show\n",
    "    8. S(v): dish-make concave; shape like a dish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Your Turn (page 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out NLTK’s convenient graphical WordNet browser:\n",
    "nltk.app.wordnet(). Explore the WordNet hierarchy by following the\n",
    "hypernym and hyponym links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.app.wordnet_app.app()>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.app.wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dark.n.01'),\n",
       " Synset('iniquity.n.01'),\n",
       " Synset('darkness.n.02'),\n",
       " Synset('night.n.01'),\n",
       " Synset('dark.n.05'),\n",
       " Synset('dark.a.01'),\n",
       " Synset('dark.a.02'),\n",
       " Synset('dark.s.03'),\n",
       " Synset('black.s.05'),\n",
       " Synset('dark.s.05'),\n",
       " Synset('dark.s.06'),\n",
       " Synset('benighted.s.02'),\n",
       " Synset('dark.s.08'),\n",
       " Synset('blue.s.08'),\n",
       " Synset('colored.s.02'),\n",
       " Synset('dark.s.11')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words of specific meaning for dark: \n",
      "['total_darkness', 'lightlessness', 'blackness', 'pitch_blackness', 'black']\n"
     ]
    }
   ],
   "source": [
    "#LemNam = wn.synset('dark.n.01').lemma_names()\n",
    "drk = wn.synsets('dark')[0] #where the array value can vary\n",
    "hyponyms = drk.hyponyms()\n",
    "#print changes; depening on array value of hyponyms\n",
    "print(\"Some words of specific meanings of dark: \")\n",
    "print(hyponyms[3].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words with a broader meaning of dark: \n",
      "['illumination']\n"
     ]
    }
   ],
   "source": [
    "hypernyms = drk.hypernyms()\n",
    "print(\"Some words with a broader meaning of dark: \")\n",
    "print(hypernyms[0].lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Exercise 2.8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('tree.n.01'), Synset('tree.n.02'), Synset('tree.n.03'), Synset('corner.v.02'), Synset('tree.v.02'), Synset('tree.v.03'), Synset('tree.v.04')]\n"
     ]
    }
   ],
   "source": [
    "t = wn.synsets('tree')\n",
    "print(t)\n",
    "tree = wn.synset('tree.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part:  [Synset('burl.n.02'), Synset('crown.n.07'), Synset('limb.n.02'), Synset('stump.n.01'), Synset('trunk.n.01')]\n",
      "Subst:  [Synset('heartwood.n.01'), Synset('sapwood.n.01')]\n",
      "Member_Holo:  [Synset('forest.n.01')]\n",
      "Member_Mero: []\n"
     ]
    }
   ],
   "source": [
    "part = tree.part_meronyms()\n",
    "subst = tree.substance_meronyms()\n",
    "m = tree.member_meronyms()\n",
    "mem = tree.member_holonyms()\n",
    "print(\"Part: \" , part)\n",
    "print(\"Subst: \", subst)\n",
    "print(\"Member_Holo: \" , mem)\n",
    "print(\"Member_Mero:\", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Exercise 2.8.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **supergloss(s)** that takes a synset **s** as its argument and returns a string consisting of the concatenation of the definition of **s**, and the definitions of all the hypernyms and hyponyms of **s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Exercise 2.8.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polysemy of a word is the number of senses it has. Using WordNet, we can\n",
    "determine that the noun *dog* has seven senses with **len(wn.synsets('dog', 'n'))**.\n",
    "Compute the average polysemy of nouns, verbs, adjectives, and adverbs according\n",
    "to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the Longest String in Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "260819\n",
      "to\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gutenberg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7ed9ed44bdbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#So this creates an array of arrays, where the inner array contains the words present in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mMoby_Dick_Sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'melville-moby_dick.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMoby_Dick_Sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlongest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gutenberg' is not defined"
     ]
    }
   ],
   "source": [
    "#CITE: https://www.nltk.org/book/ch02.html\n",
    "MB = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "print(type(MB))\n",
    "print(len(MB))\n",
    "print(MB[101])\n",
    "#for i in range(len(MB)):\n",
    "    #if (MB[i] == '.'):\n",
    "        #print('RAWR')\n",
    "#Can use bottom for easy way of finding sentences\n",
    "#macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "#So this creates an array of arrays, where the inner array contains the words present in the sentence\n",
    "\n",
    "Moby_Dick_Sent = gutenberg.sents('melville-moby_dick.txt')\n",
    "print(len(Moby_Dick_Sent))\n",
    "longest = 0\n",
    "for i in Moby_Dick_Sent:\n",
    "    if (len(i) >= longest):\n",
    "        longest = len(i)\n",
    "        LongSent = i;\n",
    "\n",
    "#print(LongSent)\n",
    "#print(len(LongSent))\n",
    "ans = ' '.join(LongSent)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Search Perplixity measures in Python and compare it to Lexical Diversity. What do each measure , are they related somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.502044830977896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The lexical Diversity \n",
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))\n",
    "lexical_diversity(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/33266956/nltk-package-to-estimate-the-unigram-perplexity\n",
    "\n",
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "MB = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "tokens = nltk.word_tokenize(str(MB))\n",
    "\n",
    "def unigram(tokens):\n",
    "    model = collections.defaultdict(lambda: 0.1)\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model[f] = 1\n",
    "            continue\n",
    "    N = float(sum(model.values()))\n",
    "    for word in model:\n",
    "        model[word] = model[word]/N\n",
    "    print (model)\n",
    "    return model\n",
    "#here you construct the unigram language model \n",
    "\n",
    "\n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function unigram.<locals>.<lambda> at 0x1a1a2dc950>, {'[': 0.09130434782608693, \"'\": 0.3086956521739129, ',': 0.26521739130434774, \"'Moby\": 0.04782608695652173, \"'Dick\": 0.04782608695652173, \"'by\": 0.04782608695652173, \"'Herman\": 0.04782608695652173, \"'Melville\": 0.04782608695652173, '...': 0.04782608695652173, ']': 0.04782608695652173})\n",
      "9.999999999999998\n"
     ]
    }
   ],
   "source": [
    "model = unigram(tokens)\n",
    "print (perplexity(str(MB), model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Put Discussion here on number 4What "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity vs Lexical Diversity Discussion:\n",
    "Page 45 - Perplexity\n",
    "Page 9 - Lexical\n",
    "Search relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TESTING SOMETHING OUT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

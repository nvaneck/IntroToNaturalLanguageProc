{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A02: Words, Transducers, Language Models, and N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    By Besnik Balaj & Amanda Ly\n",
    "I plege my honor that I have abided by the Stevens Honor System \n",
    "                      -Amanda Ly & Besnik Balaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deliverables:\n",
    "\n",
    "1. Respond to J&M 2nd Exercises 3.6, 3.8, and 4.3. You are welcome to use existing n-gram programs. If you can find packages and procedures in software that apply you can use them but report on where you got them.\n",
    "2. From BKL for Python 3 (online) submit the results of the Your Turn practice on p 69 and 70 and Exercise 2.8.5, 2.8.14 (possibly used in disambiguation), and 2.8.27 (for only verbs) on pages 74-77. The concordance tool was first used on p4. You should become quite familiar with the use of WordNet and its companion SentiWordNet.\n",
    "3. For later work relevant to Chapters 10-13 on part-of-speech parsing find the longest sentence in one of the corpora: Moby Dick, Sense and Sensibility, Inaugural Address Corpus, The Wall Street Journal, and The Man Who Was Thursday, downloaded from instructions on page 2 and 3 of BKL. In effect find the longest string between consecutive periods. \n",
    "4. Search for perplexity measures (Section 3.7 in J&M 3rd) in Python and compare perplexity to lexical diversity, as specified on page 9 of BKL, on the corpus downloaded from the Python NLTK in the previous exercise. What does each measure? Is there a potential relationship and, if so, what is it? Is there a relationship of these notions to information content? This is not a pure programming exercise. You should interpret what each means in your own words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. J&M 2nd Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.6\n",
    "Read Porter (1980) or see Martin Poerter's official homepage on the porter stemmer. \n",
    "Implement one of the steps of the Porter Stemmer as a transducer.\n",
    "\n",
    "    **See handwritten notes. for this case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.8\n",
    "Write a program that takes a word and, using an on-line dictionary, \n",
    "computes possible anagrams of the word, each of which is a legal word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def perms(elements):\n",
    "    if len(elements) <=1:\n",
    "        return elements\n",
    "    else:\n",
    "        tmp = []\n",
    "        for perm in perms(elements[1:]):\n",
    "            for i in range(len(elements)):\n",
    "                tmp.append(perm[:i] + elements[0:1] + perm[i:])\n",
    "        return tmp\n",
    "\n",
    "def wordGram(word):\n",
    "    legalWrds = []\n",
    "    #given a word make as many legal anagrams\n",
    "    possibleWords = perms(word)\n",
    "    for i in possibleWords:\n",
    "        if wordnet.synsets(i):\n",
    "            legalWrds.append(i)\n",
    "    print(legalWrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hell', 'Hell']\n"
     ]
    }
   ],
   "source": [
    "wordGram('Hell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Exercise 4.3\n",
    "Run your N-gram program on two different small corpora of your choice \n",
    "(you might use email text or newsgroups). Now compare the statistics of the two corpora. \n",
    "What are the differences in the most common unigrams between the two? \n",
    "How about interesting differences in bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This corpus is just a long text I am using from a email\n",
    "\n",
    "testEmail = \"Hi, Designers and product visionaries both ask the same question: How can I go from prototype to finished, polished product?We’d like to help if we can. Fluid UI has partnered with Developerfair.com to create an interactive tool which will guide you through some basic questions to help you get a sense for your project. Fill out the super quick questionnaire and we promise to follow up with a personalised reply within 24-72 hours with a list of tips for your project.We\\'ll also suggesting some possible next steps, including the kind of resources you might need and how much it is likely to cost.\"\n",
    "\n",
    "testEmail2 = \"GraphConnect 2020 is just around the corner (April 20-22 in New York City), and we’re calling on all graphistas to submit their presentation ideas to share with attendees. The Neo4j community has connected so many people who use graphs in incredible ways. Now it’s your turn to share your experiences with others. We look forward to reading your proposals! Thanks,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtained from StackOverflow here\n",
    "#https://stackoverflow.com/questions/13423919/computing-n-grams-using-python\n",
    "def ngrams2(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = {}\n",
    "    for i in range(len(input)-n+1):\n",
    "        g = ' '.join(input[i:i+n])\n",
    "        output.setdefault(g,0)\n",
    "        output[g] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<Text: Moby': 1,\n",
       " 'Moby Dick': 1,\n",
       " 'Dick by': 1,\n",
       " 'by Herman': 1,\n",
       " 'Herman Melville': 1,\n",
       " 'Melville 1851>': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2(str(text1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hi, Designers': 1,\n",
       " 'Designers and': 1,\n",
       " 'and product': 1,\n",
       " 'product visionaries': 1,\n",
       " 'visionaries both': 1,\n",
       " 'both ask': 1,\n",
       " 'ask the': 1,\n",
       " 'the same': 1,\n",
       " 'same question:': 1,\n",
       " 'question: How': 1,\n",
       " 'How can': 1,\n",
       " 'can I': 1,\n",
       " 'I go': 1,\n",
       " 'go from': 1,\n",
       " 'from prototype': 1,\n",
       " 'prototype to': 1,\n",
       " 'to finished,': 1,\n",
       " 'finished, polished': 1,\n",
       " 'polished product?We’d': 1,\n",
       " 'product?We’d like': 1,\n",
       " 'like to': 1,\n",
       " 'to help': 2,\n",
       " 'help if': 1,\n",
       " 'if we': 1,\n",
       " 'we can.': 1,\n",
       " 'can. Fluid': 1,\n",
       " 'Fluid UI': 1,\n",
       " 'UI has': 1,\n",
       " 'has partnered': 1,\n",
       " 'partnered with': 1,\n",
       " 'with Developerfair.com': 1,\n",
       " 'Developerfair.com to': 1,\n",
       " 'to create': 1,\n",
       " 'create an': 1,\n",
       " 'an interactive': 1,\n",
       " 'interactive tool': 1,\n",
       " 'tool which': 1,\n",
       " 'which will': 1,\n",
       " 'will guide': 1,\n",
       " 'guide you': 1,\n",
       " 'you through': 1,\n",
       " 'through some': 1,\n",
       " 'some basic': 1,\n",
       " 'basic questions': 1,\n",
       " 'questions to': 1,\n",
       " 'help you': 1,\n",
       " 'you get': 1,\n",
       " 'get a': 1,\n",
       " 'a sense': 1,\n",
       " 'sense for': 1,\n",
       " 'for your': 2,\n",
       " 'your project.': 1,\n",
       " 'project. Fill': 1,\n",
       " 'Fill out': 1,\n",
       " 'out the': 1,\n",
       " 'the super': 1,\n",
       " 'super quick': 1,\n",
       " 'quick questionnaire': 1,\n",
       " 'questionnaire and': 1,\n",
       " 'and we': 1,\n",
       " 'we promise': 1,\n",
       " 'promise to': 1,\n",
       " 'to follow': 1,\n",
       " 'follow up': 1,\n",
       " 'up with': 1,\n",
       " 'with a': 2,\n",
       " 'a personalised': 1,\n",
       " 'personalised reply': 1,\n",
       " 'reply within': 1,\n",
       " 'within 24-72': 1,\n",
       " '24-72 hours': 1,\n",
       " 'hours with': 1,\n",
       " 'a list': 1,\n",
       " 'list of': 1,\n",
       " 'of tips': 1,\n",
       " 'tips for': 1,\n",
       " \"your project.We'll\": 1,\n",
       " \"project.We'll also\": 1,\n",
       " 'also suggesting': 1,\n",
       " 'suggesting some': 1,\n",
       " 'some possible': 1,\n",
       " 'possible next': 1,\n",
       " 'next steps,': 1,\n",
       " 'steps, including': 1,\n",
       " 'including the': 1,\n",
       " 'the kind': 1,\n",
       " 'kind of': 1,\n",
       " 'of resources': 1,\n",
       " 'resources you': 1,\n",
       " 'you might': 1,\n",
       " 'might need': 1,\n",
       " 'need and': 1,\n",
       " 'and how': 1,\n",
       " 'how much': 1,\n",
       " 'much it': 1,\n",
       " 'it is': 1,\n",
       " 'is likely': 1,\n",
       " 'likely to': 1,\n",
       " 'to cost.': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2(testEmail,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GraphConnect 2020': 1,\n",
       " '2020 is': 1,\n",
       " 'is just': 1,\n",
       " 'just around': 1,\n",
       " 'around the': 1,\n",
       " 'the corner': 1,\n",
       " 'corner (April': 1,\n",
       " '(April 20-22': 1,\n",
       " '20-22 in': 1,\n",
       " 'in New': 1,\n",
       " 'New York': 1,\n",
       " 'York City),': 1,\n",
       " 'City), and': 1,\n",
       " 'and we’re': 1,\n",
       " 'we’re calling': 1,\n",
       " 'calling on': 1,\n",
       " 'on all': 1,\n",
       " 'all graphistas': 1,\n",
       " 'graphistas to': 1,\n",
       " 'to submit': 1,\n",
       " 'submit their': 1,\n",
       " 'their presentation': 1,\n",
       " 'presentation ideas': 1,\n",
       " 'ideas to': 1,\n",
       " 'to share': 2,\n",
       " 'share with': 1,\n",
       " 'with attendees.': 1,\n",
       " 'attendees. The': 1,\n",
       " 'The Neo4j': 1,\n",
       " 'Neo4j community': 1,\n",
       " 'community has': 1,\n",
       " 'has connected': 1,\n",
       " 'connected so': 1,\n",
       " 'so many': 1,\n",
       " 'many people': 1,\n",
       " 'people who': 1,\n",
       " 'who use': 1,\n",
       " 'use graphs': 1,\n",
       " 'graphs in': 1,\n",
       " 'in incredible': 1,\n",
       " 'incredible ways.': 1,\n",
       " 'ways. Now': 1,\n",
       " 'Now it’s': 1,\n",
       " 'it’s your': 1,\n",
       " 'your turn': 1,\n",
       " 'turn to': 1,\n",
       " 'share your': 1,\n",
       " 'your experiences': 1,\n",
       " 'experiences with': 1,\n",
       " 'with others.': 1,\n",
       " 'others. We': 1,\n",
       " 'We look': 1,\n",
       " 'look forward': 1,\n",
       " 'forward to': 1,\n",
       " 'to reading': 1,\n",
       " 'reading your': 1,\n",
       " 'your proposals!': 1,\n",
       " 'proposals! Thanks,': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams2(testEmail2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['[', 'Moby',\": 1,\n",
       " \"'Moby', 'Dick',\": 1,\n",
       " \"'Dick', 'by',\": 1,\n",
       " \"'by', 'Herman',\": 1,\n",
       " \"'Herman', 'Melville',\": 1,\n",
       " \"'Melville', ...]\": 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MB = str(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "ngrams2(MB,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams2(str(gutenberg.sents('shakespeare-macbeth.txt')),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi,', 'Designers')\n",
      "('Designers', 'and')\n",
      "('and', 'product')\n",
      "('product', 'visionaries')\n",
      "('visionaries', 'both')\n",
      "('both', 'ask')\n",
      "('ask', 'the')\n",
      "('the', 'same')\n",
      "('same', 'question:')\n",
      "('question:', 'How')\n",
      "('How', 'can')\n",
      "('can', 'I')\n",
      "('I', 'go')\n",
      "('go', 'from')\n",
      "('from', 'prototype')\n",
      "('prototype', 'to')\n",
      "('to', 'finished,')\n",
      "('finished,', 'polished')\n",
      "('polished', 'product?We’d')\n",
      "('product?We’d', 'like')\n",
      "('like', 'to')\n",
      "('to', 'help')\n",
      "('help', 'if')\n",
      "('if', 'we')\n",
      "('we', 'can.')\n",
      "('can.', 'Fluid')\n",
      "('Fluid', 'UI')\n",
      "('UI', 'has')\n",
      "('has', 'partnered')\n",
      "('partnered', 'with')\n",
      "('with', 'Developerfair.com')\n",
      "('Developerfair.com', 'to')\n",
      "('to', 'create')\n",
      "('create', 'an')\n",
      "('an', 'interactive')\n",
      "('interactive', 'tool')\n",
      "('tool', 'which')\n",
      "('which', 'will')\n",
      "('will', 'guide')\n",
      "('guide', 'you')\n",
      "('you', 'through')\n",
      "('through', 'some')\n",
      "('some', 'basic')\n",
      "('basic', 'questions')\n",
      "('questions', 'to')\n",
      "('to', 'help')\n",
      "('help', 'you')\n",
      "('you', 'get')\n",
      "('get', 'a')\n",
      "('a', 'sense')\n",
      "('sense', 'for')\n",
      "('for', 'your')\n",
      "('your', 'project.')\n",
      "('project.', 'Fill')\n",
      "('Fill', 'out')\n",
      "('out', 'the')\n",
      "('the', 'super')\n",
      "('super', 'quick')\n",
      "('quick', 'questionnaire')\n",
      "('questionnaire', 'and')\n",
      "('and', 'we')\n",
      "('we', 'promise')\n",
      "('promise', 'to')\n",
      "('to', 'follow')\n",
      "('follow', 'up')\n",
      "('up', 'with')\n",
      "('with', 'a')\n",
      "('a', 'personalised')\n",
      "('personalised', 'reply')\n",
      "('reply', 'within')\n",
      "('within', '24-72')\n",
      "('24-72', 'hours')\n",
      "('hours', 'with')\n",
      "('with', 'a')\n",
      "('a', 'list')\n",
      "('list', 'of')\n",
      "('of', 'tips')\n",
      "('tips', 'for')\n",
      "('for', 'your')\n",
      "('your', \"project.We'll\")\n",
      "(\"project.We'll\", 'also')\n",
      "('also', 'suggesting')\n",
      "('suggesting', 'some')\n",
      "('some', 'possible')\n",
      "('possible', 'next')\n",
      "('next', 'steps,')\n",
      "('steps,', 'including')\n",
      "('including', 'the')\n",
      "('the', 'kind')\n",
      "('kind', 'of')\n",
      "('of', 'resources')\n",
      "('resources', 'you')\n",
      "('you', 'might')\n",
      "('might', 'need')\n",
      "('need', 'and')\n",
      "('and', 'how')\n",
      "('how', 'much')\n",
      "('much', 'it')\n",
      "('it', 'is')\n",
      "('is', 'likely')\n",
      "('likely', 'to')\n",
      "('to', 'cost.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = ngrams(testEmail.split(), 2)\n",
    "unigrams = ngrams(testEmail.split(), 1)\n",
    "for grams in bigrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi,',)\n",
      "('Designers',)\n",
      "('and',)\n",
      "('product',)\n",
      "('visionaries',)\n",
      "('both',)\n",
      "('ask',)\n",
      "('the',)\n",
      "('same',)\n",
      "('question:',)\n",
      "('How',)\n",
      "('can',)\n",
      "('I',)\n",
      "('go',)\n",
      "('from',)\n",
      "('prototype',)\n",
      "('to',)\n",
      "('finished,',)\n",
      "('polished',)\n",
      "('product?We’d',)\n",
      "('like',)\n",
      "('to',)\n",
      "('help',)\n",
      "('if',)\n",
      "('we',)\n",
      "('can.',)\n",
      "('Fluid',)\n",
      "('UI',)\n",
      "('has',)\n",
      "('partnered',)\n",
      "('with',)\n",
      "('Developerfair.com',)\n",
      "('to',)\n",
      "('create',)\n",
      "('an',)\n",
      "('interactive',)\n",
      "('tool',)\n",
      "('which',)\n",
      "('will',)\n",
      "('guide',)\n",
      "('you',)\n",
      "('through',)\n",
      "('some',)\n",
      "('basic',)\n",
      "('questions',)\n",
      "('to',)\n",
      "('help',)\n",
      "('you',)\n",
      "('get',)\n",
      "('a',)\n",
      "('sense',)\n",
      "('for',)\n",
      "('your',)\n",
      "('project.',)\n",
      "('Fill',)\n",
      "('out',)\n",
      "('the',)\n",
      "('super',)\n",
      "('quick',)\n",
      "('questionnaire',)\n",
      "('and',)\n",
      "('we',)\n",
      "('promise',)\n",
      "('to',)\n",
      "('follow',)\n",
      "('up',)\n",
      "('with',)\n",
      "('a',)\n",
      "('personalised',)\n",
      "('reply',)\n",
      "('within',)\n",
      "('24-72',)\n",
      "('hours',)\n",
      "('with',)\n",
      "('a',)\n",
      "('list',)\n",
      "('of',)\n",
      "('tips',)\n",
      "('for',)\n",
      "('your',)\n",
      "(\"project.We'll\",)\n",
      "('also',)\n",
      "('suggesting',)\n",
      "('some',)\n",
      "('possible',)\n",
      "('next',)\n",
      "('steps,',)\n",
      "('including',)\n",
      "('the',)\n",
      "('kind',)\n",
      "('of',)\n",
      "('resources',)\n",
      "('you',)\n",
      "('might',)\n",
      "('need',)\n",
      "('and',)\n",
      "('how',)\n",
      "('much',)\n",
      "('it',)\n",
      "('is',)\n",
      "('likely',)\n",
      "('to',)\n",
      "('cost.',)\n"
     ]
    }
   ],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GraphConnect', '2020')\n",
      "('2020', 'is')\n",
      "('is', 'just')\n",
      "('just', 'around')\n",
      "('around', 'the')\n",
      "('the', 'corner')\n",
      "('corner', '(April')\n",
      "('(April', '20-22')\n",
      "('20-22', 'in')\n",
      "('in', 'New')\n",
      "('New', 'York')\n",
      "('York', 'City),')\n",
      "('City),', 'and')\n",
      "('and', 'we’re')\n",
      "('we’re', 'calling')\n",
      "('calling', 'on')\n",
      "('on', 'all')\n",
      "('all', 'graphistas')\n",
      "('graphistas', 'to')\n",
      "('to', 'submit')\n",
      "('submit', 'their')\n",
      "('their', 'presentation')\n",
      "('presentation', 'ideas')\n",
      "('ideas', 'to')\n",
      "('to', 'share')\n",
      "('share', 'with')\n",
      "('with', 'attendees.')\n",
      "('attendees.', 'The')\n",
      "('The', 'Neo4j')\n",
      "('Neo4j', 'community')\n",
      "('community', 'has')\n",
      "('has', 'connected')\n",
      "('connected', 'so')\n",
      "('so', 'many')\n",
      "('many', 'people')\n",
      "('people', 'who')\n",
      "('who', 'use')\n",
      "('use', 'graphs')\n",
      "('graphs', 'in')\n",
      "('in', 'incredible')\n",
      "('incredible', 'ways.')\n",
      "('ways.', 'Now')\n",
      "('Now', 'it’s')\n",
      "('it’s', 'your')\n",
      "('your', 'turn')\n",
      "('turn', 'to')\n",
      "('to', 'share')\n",
      "('share', 'your')\n",
      "('your', 'experiences')\n",
      "('experiences', 'with')\n",
      "('with', 'others.')\n",
      "('others.', 'We')\n",
      "('We', 'look')\n",
      "('look', 'forward')\n",
      "('forward', 'to')\n",
      "('to', 'reading')\n",
      "('reading', 'your')\n",
      "('your', 'proposals!')\n",
      "('proposals!', 'Thanks,')\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(testEmail2.split(), 2)\n",
    "unigrams = ngrams(testEmail2.split(), 1)\n",
    "for grams in bigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GraphConnect',)\n",
      "('2020',)\n",
      "('is',)\n",
      "('just',)\n",
      "('around',)\n",
      "('the',)\n",
      "('corner',)\n",
      "('(April',)\n",
      "('20-22',)\n",
      "('in',)\n",
      "('New',)\n",
      "('York',)\n",
      "('City),',)\n",
      "('and',)\n",
      "('we’re',)\n",
      "('calling',)\n",
      "('on',)\n",
      "('all',)\n",
      "('graphistas',)\n",
      "('to',)\n",
      "('submit',)\n",
      "('their',)\n",
      "('presentation',)\n",
      "('ideas',)\n",
      "('to',)\n",
      "('share',)\n",
      "('with',)\n",
      "('attendees.',)\n",
      "('The',)\n",
      "('Neo4j',)\n",
      "('community',)\n",
      "('has',)\n",
      "('connected',)\n",
      "('so',)\n",
      "('many',)\n",
      "('people',)\n",
      "('who',)\n",
      "('use',)\n",
      "('graphs',)\n",
      "('in',)\n",
      "('incredible',)\n",
      "('ways.',)\n",
      "('Now',)\n",
      "('it’s',)\n",
      "('your',)\n",
      "('turn',)\n",
      "('to',)\n",
      "('share',)\n",
      "('your',)\n",
      "('experiences',)\n",
      "('with',)\n",
      "('others.',)\n",
      "('We',)\n",
      "('look',)\n",
      "('forward',)\n",
      "('to',)\n",
      "('reading',)\n",
      "('your',)\n",
      "('proposals!',)\n",
      "('Thanks,',)\n"
     ]
    }
   ],
   "source": [
    "for grams in unigrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Discussion:\n",
    "Depending on the corpus being used can change both the unigram and bigram amount. Of course with regards to the unigrams\n",
    "we can see the use of common words such as the/to/a/of/etc. But its with some other words we can see differences. Neo4j is not\n",
    "Fluid UI. This is where we can see some specific choices leading to guess what corpus came from. With regards to bigrams\n",
    "A bigram can help express some matter of the field of the topic for both. Both try to attract the user to their own\n",
    "thing but in a different way. The words found in the text2 are more leanning to bring someone somewhere while text1 was\n",
    "to send a mission statement. With a larger corpus, we can see the numbers behind some cases but acquiring those have proven difficult.\n",
    "\n",
    "UPDATE:\n",
    "I did a quick run on Moby Dick as seen above and noticed that some wordplay on the bigrams are\n",
    "time dependent or social dependent. What I mean is that Shakespeare follow up words\n",
    "are going to be different from Melville's due to the time period since some words existed or\n",
    "meant different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Your Turn (page 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down all the senses of the word *dish* that you can\n",
    "think of. Now, explore this word with the help of WordNet, using the\n",
    "same operations shown earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Definitions: \n",
    "   1. dish(v) - the act of putting food onto a plate\n",
    "   2. dish(n) - a container to hold food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma_names:  ['dish']\n",
      "Definition:  a particular item of prepared food\n",
      "Examples:  ['she prepared a special dish for dinner']\n"
     ]
    }
   ],
   "source": [
    "#change the part in the parenthesis to the diff Synset options\n",
    "lemName = wn.synset('dish.n.02').lemma_names()\n",
    "defs = wn.synset('dish.n.02').definition()\n",
    "examples = wn.synset('dish.n.02').examples()\n",
    "print(\"Lemma_names: \", lemName)\n",
    "print(\"Definition: \", defs)\n",
    "print(\"Examples: \", examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.05.dish'),\n",
       " Lemma('dish.n.05.dish_aerial'),\n",
       " Lemma('dish.n.05.dish_antenna'),\n",
       " Lemma('dish.n.05.saucer')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the part in the parethesis to different Synset options \n",
    "wn.synset('dish.n.05').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('dish.n.05.dish_aerial')\n",
      "Synset('dish.n.05')\n",
      "dish_aerial\n"
     ]
    }
   ],
   "source": [
    "#Change the part in the parethesis to different Lemma options \n",
    "Lemma = wn.lemma('dish.n.05.dish_aerial')\n",
    "lemSyn = wn.lemma('dish.n.05.dish_aerial').synset()\n",
    "lemName = wn.lemma('dish.n.05.dish_aerial').name()\n",
    "print(Lemma)\n",
    "print(lemSyn)\n",
    "print(lemName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish']\n",
      "['dish']\n",
      "['dish', 'dishful']\n",
      "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
      "['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
      "['cup_of_tea', 'bag', 'dish']\n",
      "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
      "['dish']\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('dish'):\n",
    "    print (synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.01.dish'),\n",
       " Lemma('dish.n.02.dish'),\n",
       " Lemma('dish.n.03.dish'),\n",
       " Lemma('smasher.n.02.dish'),\n",
       " Lemma('dish.n.05.dish'),\n",
       " Lemma('cup_of_tea.n.01.dish'),\n",
       " Lemma('serve.v.06.dish'),\n",
       " Lemma('dish.v.02.dish')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('dish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet Definitions: \n",
    "    1. S(n): dish-a piece of dishware normally used as a container for holding or serving food\n",
    "        e.g. we gave them a set of dishes for a wedding present    \n",
    "    2. S(n): dish-a particular item of prepared food\n",
    "        e.g. she prepared a special dish for dinner\n",
    "    3. S(n): dish/dishful-the quantity that a dish will hold\n",
    "        e.g. they served me a dish of rice\n",
    "    4. S(n): smasher/stunner/knockout/beauty/ravisher/sweetheart/peach/lulu/looker/mantrap/dish-a very attractive or seductive looking woman\n",
    "    5. S(n): dish/dish_aerial/dish_antenna/saucer-directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
    "    6. S(n): cup_of_tea/bag/dish-an activity that you like or at which you are superior\n",
    "        e.g. chemistry is not my cup of tea; his bag now is learning to play golf; marriage was scarcely his dish\n",
    "    7. S(v): serve/serve_up/dish_out/dish_up/dish-provide (usually but not necessarily food)\n",
    "        e.g. We serve meals for the homeless; She dished out the soup at 8 P.M.; The entertainers served up a lively show\n",
    "    8. S(v): dish-make concave; shape like a dish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Your Turn (page 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out NLTK’s convenient graphical WordNet browser:\n",
    "nltk.app.wordnet(). Explore the WordNet hierarchy by following the\n",
    "hypernym and hyponym links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.app.wordnet_app.app()>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.app.wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dark.n.01'),\n",
       " Synset('iniquity.n.01'),\n",
       " Synset('darkness.n.02'),\n",
       " Synset('night.n.01'),\n",
       " Synset('dark.n.05'),\n",
       " Synset('dark.a.01'),\n",
       " Synset('dark.a.02'),\n",
       " Synset('dark.s.03'),\n",
       " Synset('black.s.05'),\n",
       " Synset('dark.s.05'),\n",
       " Synset('dark.s.06'),\n",
       " Synset('benighted.s.02'),\n",
       " Synset('dark.s.08'),\n",
       " Synset('blue.s.08'),\n",
       " Synset('colored.s.02'),\n",
       " Synset('dark.s.11')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words of specific meanings, aka hyponyms, of dark: \n",
      "['total_darkness', 'lightlessness', 'blackness', 'pitch_blackness', 'black']\n"
     ]
    }
   ],
   "source": [
    "#LemNam = wn.synset('dark.n.01').lemma_names()\n",
    "drk = wn.synsets('dark')[0] #where the array value can vary\n",
    "hyponyms = drk.hyponyms()\n",
    "#print changes; depening on array value of hyponyms\n",
    "print(\"Some words of specific meanings, aka hyponyms, of dark: \")\n",
    "print(hyponyms[3].lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words with a broader meaning, aka hypernyms, of dark: \n",
      "['illumination']\n"
     ]
    }
   ],
   "source": [
    "hypernyms = drk.hypernyms()\n",
    "print(\"Some words with a broader meaning, aka hypernyms, of dark: \")\n",
    "print(hypernyms[0].lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Exercise 2.8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: **member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(),** and **substance_holonyms().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = wn.synsets('head')\n",
    "t2 = wn.synsets('duck')\n",
    "t3 = wn.synsets('water')\n",
    "t4 = wn.synsets('forest')\n",
    "tst = wn.synset('head.n.01')\n",
    "tst2 = wn.synset('duck.n.01')\n",
    "tst3 = wn.synset('water.n.01')\n",
    "tst4 = wn.synset('forest.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meroynms\n",
      "-------------\n",
      "1. Part Meroynms:  [Synset('basilar_artery.n.01'), Synset('brain.n.01'), Synset('ear.n.01'), Synset('face.n.01'), Synset('muzzle.n.02'), Synset('skull.n.01'), Synset('temple.n.02')]\n",
      "2. Substance Meronyms:  [Synset('hydrogen.n.01'), Synset('oxygen.n.01')]\n",
      "3. Member_Meronyms: [Synset('tree.n.01'), Synset('underbrush.n.01')]\n",
      "\n",
      "Holonyms\n",
      "-------------\n",
      "1. Member_Holonyms:  [Synset('anatidae.n.01')]\n",
      "2. Substance_Holonyms:  [Synset('body_of_water.n.01'), Synset('ice.n.01'), Synset('ice_crystal.n.01'), Synset('perspiration.n.01'), Synset('snowflake.n.01'), Synset('tear.n.01')]\n",
      "3. Part_Holonyms:  [Synset('animal.n.01'), Synset('body.n.01')]\n"
     ]
    }
   ],
   "source": [
    "part = tst.part_meronyms()\n",
    "subst = tst3.substance_meronyms()\n",
    "m = tst4.member_meronyms()\n",
    "mem = tst2.member_holonyms()\n",
    "sub = tst3.substance_holonyms()\n",
    "pt = tst.part_holonyms()\n",
    "print(\"Meroynms\\n-------------\\n1. Part Meroynms: \" , part)\n",
    "print(\"2. Substance Meronyms: \", subst)\n",
    "print(\"3. Member_Meronyms:\", m)\n",
    "print(\"\\nHolonyms\\n-------------\\n1. Member_Holonyms: \" , mem)\n",
    "print(\"2. Substance_Holonyms: \" , sub)\n",
    "print(\"3. Part_Holonyms: \" , pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Exercise 2.8.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function **supergloss(s)** that takes a synset **s** as its argument and returns a string consisting of the concatenation of the definition of **s**, and the definitions of all the hypernyms and hyponyms of **s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supergloss(s):\n",
    "    define = \"Definition: \" + s.definition()\n",
    "    for hyponym in s.hyponyms():\n",
    "        define = define + '\\nHyponym:' + hyponym.definition()\n",
    "    for hypernym in s.hypernyms():\n",
    "        define = define + '\\nHypernym:' + hypernym.definition()\n",
    "    return define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\n",
      "Hyponym:an apple used primarily in cooking for pies and applesauce etc\n",
      "Hyponym:small sour apple; suitable for preserving\n",
      "Hyponym:an apple used primarily for eating raw without cooking\n",
      "Hypernym:edible reproductive body of a seed plant especially one having sweet flesh\n",
      "Hypernym:a fleshy fruit (apple or pear or related fruits) having seed chambers and an outer fleshy part\n"
     ]
    }
   ],
   "source": [
    "#The value in [ ] can be changed so long as its within range\n",
    "apple = wn.synsets('apple')[0]\n",
    "print(supergloss(apple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Exercise 2.8.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polysemy of a word is the number of senses it has. Using WordNet, we can\n",
    "determine that the noun *dog* has seven senses with **len(wn.synsets('dog', 'n'))**.\n",
    "Compute the average polysemy of nouns, verbs, adjectives, and adverbs according\n",
    "to WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1 0 0\n"
     ]
    }
   ],
   "source": [
    "# Number of Synset Type in Dog:\n",
    "noun = len(wn.synsets('dog','n'))\n",
    "verb = len(wn.synsets('dog','v'))\n",
    "adj = len(wn.synsets('dog','a'))\n",
    "adv = len(wn.synsets('dog','r'))\n",
    "print(noun,verb,adj,adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of nouns:  82115\n",
      "total # of verbs:  13767\n",
      "total # of adjectives:  18156\n",
      "total # of adverbs:  3621\n"
     ]
    }
   ],
   "source": [
    "#For personal view\n",
    "#Number of Synset Type according to WordNet\n",
    "N = len(list(wn.all_synsets('n')))\n",
    "V = len(list(wn.all_synsets('v')))\n",
    "J = len(list(wn.all_synsets('a')))\n",
    "D = len(list(wn.all_synsets('r')))\n",
    "print(\"total # of nouns: \", N)\n",
    "print(\"total # of verbs: \", V)\n",
    "print(\"total # of adjectives: \", J)\n",
    "print(\"total # of adverbs: \", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Polysemy of  n :  1.2833560159282222\n"
     ]
    }
   ],
   "source": [
    "# INTERACTIVE METHOD\n",
    "#Change type:'n' for nouns, 'v' for verbs, 'a' for adjectives, 'r' for adverbs\n",
    "type = 'n'\n",
    "synsets = wn.all_synsets(type)\n",
    "def avgPolysemy(SType):\n",
    "    lst = set()\n",
    "    count = 0\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            lst.add(lemma.name())\n",
    "    for lemma in lst:\n",
    "        count += len(wn.synsets(lemma, SType))\n",
    "    return count/len(lst)\n",
    "print('Average Polysemy of ', type,': ', avgPolysemy(type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Polysemy of nouns:  1.2833560159282222\n",
      "Average Polysemy of verbs:  2.1866273523545225\n",
      "Average Polysemy of adjectives:  1.4104837960813446\n",
      "Average Polysemy of adverbs:  1.2532916759651864\n"
     ]
    }
   ],
   "source": [
    "#Kind-of a Brute Force Method: (for answer display purpose)\n",
    "nouns = wn.all_synsets('n')\n",
    "verbs = wn.all_synsets('v')\n",
    "adjec = wn.all_synsets('a')\n",
    "adver = wn.all_synsets('r')\n",
    "\n",
    "\n",
    "lst = set()\n",
    "count = 0\n",
    "for synset in nouns:\n",
    "    for lemma in synset.lemmas():\n",
    "        lst.add(lemma.name())\n",
    "for lemma in lst:\n",
    "    count += len(wn.synsets(lemma, 'n'))\n",
    "print('Average Polysemy of nouns: ', count/len(lst))\n",
    "\n",
    "lst = set()\n",
    "count = 0\n",
    "for synset in verbs:\n",
    "    for lemma in synset.lemmas():\n",
    "        lst.add(lemma.name())\n",
    "for lemma in lst:\n",
    "    count += len(wn.synsets(lemma, 'v'))\n",
    "print('Average Polysemy of verbs: ', count/len(lst))\n",
    "\n",
    "lst = set()\n",
    "count = 0\n",
    "for synset in adjec:\n",
    "    for lemma in synset.lemmas():\n",
    "        lst.add(lemma.name())\n",
    "for lemma in lst:\n",
    "    count += len(wn.synsets(lemma, 'a'))\n",
    "print('Average Polysemy of adjectives: ', count/len(lst))\n",
    "\n",
    "lst = set()\n",
    "count=0\n",
    "for synset in adver:\n",
    "    for lemma in synset.lemmas():\n",
    "        lst.add(lemma.name())\n",
    "for lemma in lst:\n",
    "    count += len(wn.synsets(lemma, 'r'))\n",
    "print('Average Polysemy of adverbs: ', count/len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the Longest String in Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest sentence is: 542Words\n",
      "Though in many natural objects , whiteness refiningly enhances beauty , as if imparting some special virtue of its own , as in marbles , japonicas , and pearls ; and though various nations have in some way recognised a certain royal preeminence in this hue ; even the barbaric , grand old kings of Pegu placing the title \" Lord of the White Elephants \" above all their other magniloquent ascriptions of dominion ; and the modern kings of Siam unfurling the same snow - white quadruped in the royal standard ; and the Hanoverian flag bearing the one figure of a snow - white charger ; and the great Austrian Empire , Caesarian , heir to overlording Rome , having for the imperial colour the same imperial hue ; and though this pre - eminence in it applies to the human race itself , giving the white man ideal mastership over every dusky tribe ; and though , besides , all this , whiteness has been even made significant of gladness , for among the Romans a white stone marked a joyful day ; and though in other mortal sympathies and symbolizings , this same hue is made the emblem of many touching , noble things -- the innocence of brides , the benignity of age ; though among the Red Men of America the giving of the white belt of wampum was the deepest pledge of honour ; though in many climes , whiteness typifies the majesty of Justice in the ermine of the Judge , and contributes to the daily state of kings and queens drawn by milk - white steeds ; though even in the higher mysteries of the most august religions it has been made the symbol of the divine spotlessness and power ; by the Persian fire worshippers , the white forked flame being held the holiest on the altar ; and in the Greek mythologies , Great Jove himself being made incarnate in a snow - white bull ; and though to the noble Iroquois , the midwinter sacrifice of the sacred White Dog was by far the holiest festival of their theology , that spotless , faithful creature being held the purest envoy they could send to the Great Spirit with the annual tidings of their own fidelity ; and though directly from the Latin word for white , all Christian priests derive the name of one part of their sacred vesture , the alb or tunic , worn beneath the cassock ; and though among the holy pomps of the Romish faith , white is specially employed in the celebration of the Passion of our Lord ; though in the Vision of St . John , white robes are given to the redeemed , and the four - and - twenty elders stand clothed in white before the great - white throne , and the Holy One that sitteth there white like wool ; yet for all these accumulated associations , with whatever is sweet , and honourable , and sublime , there yet lurks an elusive something in the innermost idea of this hue , which strikes more of panic to the soul than that redness which affrights in blood .\n"
     ]
    }
   ],
   "source": [
    "#CITE: https://www.nltk.org/book/ch02.html\n",
    "MB = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "#print(type(MB))\n",
    "#print(len(MB))\n",
    "#print(MB[101])\n",
    "#for i in range(len(MB)):\n",
    "    #if (MB[i] == '.'):\n",
    "        #print('RAWR')\n",
    "#Can use bottom for easy way of finding sentences\n",
    "#macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "#So this creates an array of arrays, where the inner array contains the words present in the sentence\n",
    "\n",
    "Moby_Dick_Sent = gutenberg.sents('melville-moby_dick.txt')\n",
    "#print(len(Moby_Dick_Sent))\n",
    "longest = 0\n",
    "for i in Moby_Dick_Sent:\n",
    "    if (len(i) >= longest):\n",
    "        longest = len(i)\n",
    "        LongSent = i;\n",
    "\n",
    "#print(LongSent)\n",
    "print('The length of the longest sentence is: '+ str(len(LongSent)) + 'Words')\n",
    "ans = ' '.join(LongSent)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Search Perplixity measures in Python and compare it to Lexical Diversity. What do each measure , are they related somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The lexical Diversity \n",
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))\n",
    "lexical_diversity(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/33266956/nltk-package-to-estimate-the-unigram-perplexity\n",
    "\n",
    "import collections, nltk\n",
    "# we first tokenize the text corpus\n",
    "MB = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "tokens = nltk.word_tokenize(str(MB))\n",
    "\n",
    "def unigram(tokens):\n",
    "    model = collections.defaultdict(lambda: 0.1)\n",
    "    for f in tokens:\n",
    "        try:\n",
    "            model[f] += 1\n",
    "        except KeyError:\n",
    "            model[f] = 1\n",
    "            continue\n",
    "    N = float(sum(model.values()))\n",
    "    for word in model:\n",
    "        model[word] = model[word]/N\n",
    "    print (model)\n",
    "    return model\n",
    "#here you construct the unigram language model \n",
    "\n",
    "\n",
    "def perplexity(testset, model):\n",
    "    testset = testset.split()\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in testset:\n",
    "        N += 1\n",
    "        perplexity = perplexity * (1/model[word])\n",
    "    perplexity = pow(perplexity, 1/float(N)) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unigram(tokens)\n",
    "print (perplexity(str(MB), model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Put Discussion here on number 4What "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity vs Lexical Diversity Discussion:\n",
    "Page 45 - Perplexity\n",
    "Page 9 - Lexical\n",
    "Search relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
